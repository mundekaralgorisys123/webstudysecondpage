{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7caac-d7bf-4b46-9ecb-31c72f40d5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe070d9-77cd-4678-9f1c-0dc496b7b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "https://www.kay.com/engagement/c/9000000001\n",
      "============================\n",
      "Successfully loaded: https://www.kay.com/engagement/c/9000000001?loadMore=0 | IP: 192.168.29.118\n",
      "No of products in Portal 42\n",
      "Record Number : 1\n",
      "=================== 1st phase data =============================\n",
      "Row Number: 1\n",
      "Product URL: https://www.kay.com/now-forever-ovalcut-diamond-engagement-ring-12-ct-tw-14k-yellow-gold/p/V-142107616\n",
      "Product Name: Now + Forever Oval-Cut Diamond Engagement Ring 1/2 ct tw 14K Yellow Gold\n",
      "Online Exclusive\n",
      "Price: $1,699.99\n",
      "Image URL: https://www.kay.com/productimages/processed/V-142107616_0_260.jpg?pristine=true\n",
      "Additional Info (Raw Text or JSON): Online Exclusive\n",
      "================================================\n",
      "Button not found or already closed.\n",
      "==================== second phase data IN ===========\n",
      "title: Now + Forever Oval-Cut Diamond Engagement Ring 1/2 ct tw 14K Yellow Gold\n",
      "sku: 142107616\n",
      "ring_sizes: ['5', '6', '7', '8', '9']\n",
      "final_output_price: Discounted Price: Discounted Price $1,699.99 | Original Price: N/A | Discount: N/A\n",
      "protection_plan: {\"title\": \"\", \"subtitle\": \"\", \"services\": []}\n",
      "monthly_payment: N/A\n",
      "product_details: {'error': 'No specifications found'}\n",
      "review_summary: {\"summary\": {\"text\": \"Review summary not available\", \"overall_rating\": \"N/A\"}, \"overview\": {}, \"breakdown\": {}, \"reviews\": []}\n",
      "image_urls: ['https://www.kay.com/productimages/processed/V-142107616_0_100.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-142107616_1_100.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-142107616_2_100.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-142107616_0_800.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-142107616_1_800.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-142107616_2_800.jpg?pristine=true']\n",
      "==================== second phase data OUT ===========\n",
      "Record Number : 2\n",
      "=================== 1st phase data =============================\n",
      "Row Number: 2\n",
      "Product URL: https://www.kay.com/now-forever-roundcut-labgrown-diamond-halo-engagement-ring-2-ct-tw-14k-white-gold/p/V-961595701\n",
      "Product Name: Now + Forever Round-Cut Lab-Grown Diamond Halo Engagement Ring 2 ct tw 14K White Gold\n",
      "Clearance\n",
      "Price: $2,579.99 offer of 40% $4,299.99\n",
      "Image URL: https://www.kay.com/productimages/processed/V-961595701_0_260.jpg?pristine=true\n",
      "Additional Info (Raw Text or JSON): Clearance\n",
      "================================================\n",
      "Button not found or already closed.\n",
      "==================== second phase data IN ===========\n",
      "title: Now + Forever Round-Cut Lab-Grown Diamond Halo Engagement Ring 2 ct tw 14K White Gold\n",
      "sku: 961595701\n",
      "ring_sizes: ['5', '5.25', '5.5', '5.75', '6', '6.25', '6.5', '6.75', '7', '7.25', '7.5', '7.75', '8', '8.25', '8.5', '8.75', '9']\n",
      "final_output_price: Discounted Price: Discounted Price $4,299.99 | Original Price: N/A | Discount: N/A\n",
      "protection_plan: {\"title\": \"\", \"subtitle\": \"\", \"services\": []}\n",
      "monthly_payment: N/A\n",
      "product_details: {'error': 'No specifications found'}\n",
      "review_summary: {\"summary\": {\"text\": \"Review summary not available\", \"overall_rating\": \"N/A\"}, \"overview\": {}, \"breakdown\": {}, \"reviews\": []}\n",
      "image_urls: ['https://www.kay.com/productimages/processed/V-961595701_0_100.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-961595701_2_100.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-961595701_0_800.jpg?pristine=true', 'https://www.kay.com/productimages/processed/V-961595701_2_800.jpg?pristine=true']\n",
      "==================== second phase data OUT ===========\n",
      "Record Number : 3\n",
      "=================== 1st phase data =============================\n",
      "Row Number: 3\n",
      "Product URL: https://www.kay.com/now-forever-labgrown-diamonds-roundcut-solitaire-engagement-ring-114-ct-tw-14k-white-gold-isi2/p/V-655751206\n",
      "Product Name: Now + Forever Lab-Grown Diamonds Round-Cut Solitaire Engagement Ring 1-1/4 ct tw 14K White Gold (I/SI2)\n",
      "Clearance\n",
      "Price: $1,449.99 offer of 50% $2,899.99\n",
      "Image URL: https://www.kay.com/productimages/processed/V-655751206_0_260.jpg?pristine=true\n",
      "Additional Info (Raw Text or JSON): Clearance\n",
      "================================================\n",
      "Button not found or already closed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import uuid\n",
    "import asyncio\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from playwright.async_api import async_playwright, TimeoutError, Error\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    "from utils import get_public_ip, log_event, sanitize_filename\n",
    "from dotenv import load_dotenv\n",
    "from database import insert_into_db, insert_into_db_details\n",
    "from limit_checker import update_product_count\n",
    "from io import BytesIO\n",
    "import httpx\n",
    "import traceback\n",
    "from typing import List, Tuple\n",
    "load_dotenv()\n",
    "PROXY_URL = os.getenv(\"PROXY_URL\")\n",
    "\n",
    "\n",
    "PROXY_SERVER = os.getenv(\"PROXY_SERVER\")\n",
    "PROXY_USERNAME = os.getenv(\"PROXY_USERNAME\")\n",
    "PROXY_PASSWORD = os.getenv(\"PROXY_PASSWORD\")\n",
    "\n",
    "EXCEL_DATA_PATH = \"./excel_data\"\n",
    "IMAGE_SAVE_PATH = \"./images\"\n",
    "\n",
    "\n",
    "async def download_and_resize_image(session, image_url):\n",
    "    try:\n",
    "        async with session.get(modify_image_url(image_url), timeout=10) as response:\n",
    "            if response.status != 200:\n",
    "                return None\n",
    "            content = await response.read()\n",
    "            image = PILImage.open(BytesIO(content))\n",
    "            image.thumbnail((200, 200))\n",
    "            img_byte_arr = BytesIO()\n",
    "            image.save(img_byte_arr, format='JPEG', optimize=True, quality=85)\n",
    "            return img_byte_arr.getvalue()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error downloading/resizing image: {e}\")\n",
    "        return None\n",
    "\n",
    "def modify_image_url(image_url):\n",
    "    \"\"\"Modify the image URL to replace '_260' with '_1200' while keeping query parameters.\"\"\"\n",
    "    if not image_url or image_url == \"N/A\":\n",
    "        return image_url\n",
    "\n",
    "    # Extract and preserve query parameters\n",
    "    query_params = \"\"\n",
    "    if \"?\" in image_url:\n",
    "        image_url, query_params = image_url.split(\"?\", 1)\n",
    "        query_params = f\"?{query_params}\"\n",
    "\n",
    "    # Replace '_260' with '_1200' while keeping the rest of the URL intact\n",
    "    modified_url = re.sub(r'(_260)(?=\\.\\w+$)', '_1200', image_url)\n",
    "\n",
    "    return modified_url + query_params  # Append query parameters if they exist\n",
    "\n",
    "async def download_image_async(image_url, product_name, timestamp, image_folder, unique_id, retries=3):\n",
    "    if not image_url or image_url == \"N/A\":\n",
    "        return \"N/A\"\n",
    "\n",
    "    image_filename = f\"{unique_id}_{timestamp}.jpg\"\n",
    "    image_full_path = os.path.join(image_folder, image_filename)\n",
    "    modified_url = modify_image_url(image_url)\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = await client.get(modified_url)\n",
    "                response.raise_for_status()\n",
    "                with open(image_full_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return image_full_path\n",
    "            except httpx.RequestError as e:\n",
    "                logging.warning(f\"Retry {attempt + 1}/{retries} - Error downloading {product_name}: {e}\")\n",
    "    logging.error(f\"Failed to download {product_name} after {retries} attempts.\")\n",
    "    return \"N/A\"\n",
    "\n",
    "def random_delay(min_sec=1, max_sec=3):\n",
    "    \"\"\"Introduce a random delay to mimic human-like behavior.\"\"\"\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "\n",
    "########################################  safe_goto_and_wait ####################################################################\n",
    "\n",
    "\n",
    "async def safe_goto_and_wait(page, url,isbri_data, retries=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"[Attempt {attempt + 1}] Navigating to: {url}\")\n",
    "            \n",
    "            if isbri_data:\n",
    "                await page.goto(url, timeout=180_000, wait_until=\"domcontentloaded\")\n",
    "            else:\n",
    "                await page.goto(url, wait_until=\"networkidle\", timeout=180_000)\n",
    "\n",
    "            # Wait for the selector with a longer timeout\n",
    "            product_cards = await page.wait_for_selector(\".product-scroll-wrapper\", state=\"attached\", timeout=30000)\n",
    "\n",
    "            # Optionally validate at least 1 is visible (Playwright already does this)\n",
    "            if product_cards:\n",
    "                print(\"[Success] Product cards loaded.\")\n",
    "                return\n",
    "        except Error as e:\n",
    "            logging.error(f\"Error navigating to {url} on attempt {attempt + 1}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                logging.info(\"Retrying after waiting a bit...\")\n",
    "                random_delay(1, 3)  # Add a delay before retrying\n",
    "            else:\n",
    "                logging.error(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "                raise\n",
    "        except TimeoutError as e:\n",
    "            logging.warning(f\"TimeoutError on attempt {attempt + 1} navigating to {url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                logging.info(\"Retrying after waiting a bit...\")\n",
    "                random_delay(1, 3)  # Add a delay before retrying\n",
    "            else:\n",
    "                logging.error(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "                raise\n",
    "\n",
    "\n",
    "########################################  get browser with proxy ####################################################################\n",
    "      \n",
    "\n",
    "async def get_browser_with_proxy_strategy(p, url: str):\n",
    "    \"\"\"\n",
    "    Dynamically checks robots.txt and selects proxy accordingly\n",
    "    Always uses proxies - never scrapes directly\n",
    "    \"\"\"\n",
    "    parsed_url = httpx.URL(url)\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.host}\"\n",
    "    \n",
    "    # 1. Fetch and parse robots.txt\n",
    "    disallowed_patterns = await get_robots_txt_rules(base_url)\n",
    "    \n",
    "    # 2. Check if URL matches any disallowed pattern\n",
    "    is_disallowed = check_url_against_rules(str(parsed_url), disallowed_patterns)\n",
    "    \n",
    "    # 3. Try proxies in order (bri-data first if allowed, oxylabs if disallowed)\n",
    "    proxies_to_try = [\n",
    "        PROXY_URL if not is_disallowed else {\n",
    "            \"server\": PROXY_SERVER,\n",
    "            \"username\": PROXY_USERNAME,\n",
    "            \"password\": PROXY_PASSWORD\n",
    "        },\n",
    "        {  # Fallback to the other proxy\n",
    "            \"server\": PROXY_SERVER,\n",
    "            \"username\": PROXY_USERNAME,\n",
    "            \"password\": PROXY_PASSWORD\n",
    "        } if not is_disallowed else PROXY_URL\n",
    "    ]\n",
    "\n",
    "    last_error = None\n",
    "    for proxy_config in proxies_to_try:\n",
    "        browser = None\n",
    "        try:\n",
    "            isbri_data = False\n",
    "            if proxy_config == PROXY_URL:\n",
    "                logging.info(\"Attempting with bri-data proxy (allowed by robots.txt)\")\n",
    "                print(\"Attempting with bri-data proxy (allowed by robots.txt)\")\n",
    "                browser = await p.chromium.connect_over_cdp(PROXY_URL)\n",
    "                isbri_data = True\n",
    "            else:\n",
    "                logging.info(\"Attempting with oxylabs proxy (required by robots.txt)\")\n",
    "                browser = await p.chromium.launch(\n",
    "                    proxy=proxy_config,\n",
    "                    headless=True,\n",
    "                    args=[\n",
    "                        '--disable-blink-features=AutomationControlled',\n",
    "                        '--disable-web-security'\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            context = await browser.new_context()\n",
    "            await context.add_init_script(\"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined\n",
    "                })\n",
    "            \"\"\")\n",
    "            page = await context.new_page()\n",
    "            \n",
    "            await safe_goto_and_wait(page, url,isbri_data)\n",
    "            return browser, page\n",
    "\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            error_trace = traceback.format_exc()\n",
    "            logging.error(f\"Proxy attempt failed:\\n{error_trace}\")\n",
    "            print(f\"Proxy attempt failed:\\n{error_trace}\")\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "            continue\n",
    "\n",
    "    error_msg = (f\"Failed to load {url} using all proxy options. \"\n",
    "                f\"Last error: {str(last_error)}\\n\"\n",
    "                f\"URL may be disallowed by robots.txt or proxies failed.\")\n",
    "    logging.error(error_msg)\n",
    "    print(error_msg)\n",
    "    raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def get_robots_txt_rules(base_url: str) -> List[str]:\n",
    "    \"\"\"Dynamically fetch and parse robots.txt rules\"\"\"\n",
    "    robots_url = f\"{base_url}/robots.txt\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            resp = await client.get(robots_url, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                return [\n",
    "                    line.split(\":\", 1)[1].strip()\n",
    "                    for line in resp.text.splitlines()\n",
    "                    if line.lower().startswith(\"disallow:\")\n",
    "                ]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Couldn't fetch robots.txt: {e}\")\n",
    "        print(\"Couldnt fetch robots.txt:\", {e})\n",
    "    return []\n",
    "\n",
    "\n",
    "def check_url_against_rules(url: str, disallowed_patterns: List[str]) -> bool:\n",
    "    \"\"\"Check if URL matches any robots.txt disallowed pattern\"\"\"\n",
    "    for pattern in disallowed_patterns:\n",
    "        try:\n",
    "            # Handle wildcard patterns\n",
    "            if \"*\" in pattern:\n",
    "                regex_pattern = pattern.replace(\"*\", \".*\")\n",
    "                if re.search(regex_pattern, url):\n",
    "                    return True\n",
    "            # Handle path patterns\n",
    "            elif url.startswith(f\"{pattern}\"):\n",
    "                return True\n",
    "            # Handle query parameters\n",
    "            elif (\"?\" in url) and any(\n",
    "                f\"{param}=\" in url \n",
    "                for param in pattern.split(\"=\")[0].split(\"*\")[-1:]\n",
    "                if \"=\" in pattern\n",
    "            ):\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking pattern {pattern}: {e}\")\n",
    "            \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def build_url_with_loadmore(base_url: str, page_count: int) -> str:\n",
    "    separator = '&' if '?' in base_url else '?'\n",
    "    return f\"{base_url}{separator}loadMore={page_count}\"   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "async def scrape_product_detail(url):\n",
    "    title = sku = final_output_price = protection_plan = monthly_payment = result = review_summary = \"N/A\"\n",
    "    ring_sizes = []\n",
    "    image_urls = []\n",
    "\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=False)\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "            context = await browser.new_context()\n",
    "            #context = await browser.new_context(\n",
    "            #    proxy={\n",
    "            #        \"server\": PROXY_SERVER,\n",
    "            #        \"username\": PROXY_USERNAME,\n",
    "            #        \"password\": PROXY_PASSWORD\n",
    "            #     }\n",
    "            # )\n",
    "\n",
    "            page = await context.new_page()\n",
    "            logging.info(\"Opening page...\")\n",
    "            await page.goto(url, timeout=60000)\n",
    "            logging.info(f\"Successfully accessed {url}\")\n",
    "\n",
    "            # Wait for critical elements to load\n",
    "            try:\n",
    "                logging.info(\"Scrolling from top to bottom to load all lazy-loaded content...\")\n",
    "                previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            \n",
    "                while True:\n",
    "                    await page.evaluate(\"window.scrollBy(0, 1000)\")  # scroll in steps\n",
    "                    await asyncio.sleep(0.5)  # allow lazy elements to load\n",
    "            \n",
    "                    new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                    if new_height == previous_height:\n",
    "                        break  # reached the bottom\n",
    "                    previous_height = new_height\n",
    "            \n",
    "                logging.info(\"Scrolling completed.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Scrolling failed: {e}\")\n",
    "\n",
    "            # Wait and click the \"No, thanks\" button using aria-label\n",
    "            try:\n",
    "                await page.wait_for_selector('button[aria-label=\"No, thanks; close the dialog\"]', timeout=5000)\n",
    "                await page.click('button[aria-label=\"No, thanks; close the dialog\"]')\n",
    "                print(\"Clicked the 'No, thanks' button.\")\n",
    "            except:\n",
    "                print(\"Button not found or already closed.\")\n",
    "\n",
    "\n",
    "            # SKU Extraction\n",
    "            try:\n",
    "                sku_el = await page.query_selector(\".product-detail__intro--productcode\")\n",
    "                if sku_el:\n",
    "                    text = (await sku_el.inner_text()).strip()\n",
    "                    if \"Item #:\" in text:\n",
    "                        sku = text.split(\"Item #:\")[1].strip()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[SKU Extraction Error] {e}\")\n",
    "\n",
    "            # Images\n",
    "            base_url = \"https://www.kay.com\"\n",
    "            try:\n",
    "                img_elements = await page.query_selector_all(\".swiper-slide img\")\n",
    "                for img in img_elements:\n",
    "                    src = await img.get_attribute(\"src\")\n",
    "                    if src and src.startswith(\"/productimages/processed\"):\n",
    "                        image_urls.append(base_url + src)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[Image Extraction Error] {e}\")\n",
    "\n",
    "            # Title\n",
    "            try:\n",
    "                title = await page.locator(\"div.product-detail__summary--name h1\").inner_text()\n",
    "            except:\n",
    "                title = \"N/A\"\n",
    "\n",
    "            # Prices and Discount\n",
    "            try:\n",
    "                discounted_price = await page.locator(\"span.product-price__price\").inner_text()\n",
    "            except:\n",
    "                discounted_price = \"N/A\"\n",
    "            \n",
    "            try:\n",
    "                original_price = await page.locator(\"span.product-price__striked\").inner_text()\n",
    "            except:\n",
    "                original_price = None\n",
    "            \n",
    "            try:\n",
    "                discount = await page.locator(\"span.tag-text\").inner_text()\n",
    "            except:\n",
    "                discount = None\n",
    "            \n",
    "            # Construct final string smartly\n",
    "            price_info = f\"Discounted Price: {discounted_price.strip()}\"\n",
    "            \n",
    "            price_info += f\" | Original Price: {original_price.strip() if original_price else 'N/A'}\"\n",
    "            price_info += f\" | Discount: {discount.strip() if discount else 'N/A'}\"\n",
    "            \n",
    "            final_output_price = price_info\n",
    "\n",
    "\n",
    "            try:\n",
    "                # This gets the full visible text, including amount and total\n",
    "                monthly_payment = await page.inner_text(\"div.ep-text-placement\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error extracting monthly payment: {str(e)}\")\n",
    "                monthly_payment = \"N/A\"\n",
    "\n",
    "\n",
    "            # Ring Sizes\n",
    "            try:\n",
    "                # Wait until ring size selector is loaded\n",
    "                await page.wait_for_selector(\"div.ring-size-selector label.selector-label\", timeout=10000)\n",
    "                \n",
    "                # Get all visible ring size texts\n",
    "                size_elements = await page.locator(\"div.ring-size-selector label.selector-label\").all_inner_texts()\n",
    "            \n",
    "                # Clean each size (remove asterisks and extra whitespace)\n",
    "                ring_sizes = [s.strip().replace(\"*\", \"\") for s in size_elements]\n",
    "            \n",
    "                # print(\"Extracted Ring Sizes:\", ring_sizes)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to extract ring sizes: {e}\")\n",
    "                ring_sizes = []\n",
    "\n",
    "\n",
    "            \n",
    "            # # Protection Plan\n",
    "\n",
    "            try:\n",
    "                # Extract header title and subtitle\n",
    "                title_block = await page.locator(\"div.warranty_heading span\").all_inner_texts()\n",
    "                main_title = title_block[0].strip() if len(title_block) > 0 else \"\"\n",
    "                subtitle = title_block[1].strip() if len(title_block) > 1 else \"\"\n",
    "            \n",
    "                protection_plan_data = {\n",
    "                    \"title\": main_title,\n",
    "                    \"subtitle\": subtitle,\n",
    "                    \"services\": []\n",
    "                }\n",
    "            \n",
    "                # Extract service rows\n",
    "                rows = await page.locator(\"table.table tbody tr\").all()\n",
    "                for row in rows:\n",
    "                    cols = await row.locator(\"td\").all_inner_texts()\n",
    "                    if len(cols) >= 3:\n",
    "                        protection_plan_data[\"services\"].append({\n",
    "                            \"service_needed\": cols[0].strip(),\n",
    "                            \"typical_cost\": cols[1].strip(),\n",
    "                            \"with_plan_cost\": cols[2].strip()\n",
    "                        })\n",
    "            \n",
    "                # Convert to JSON string for storage\n",
    "                protection_plan = json.dumps(protection_plan_data, ensure_ascii=False)\n",
    "            \n",
    "            except Exception as e:\n",
    "                protection_plan = json.dumps({\"error\": f\"Error extracting protection plan: {str(e)}\"})\n",
    "\n",
    "          \n",
    "            \n",
    "            # Product Specifications\n",
    "            try:\n",
    "                product_specifications = {}\n",
    "                seen_sections = set()\n",
    "            \n",
    "                # 1. Expand 'Details' accordion if collapsed\n",
    "                try:\n",
    "                    details_button = await page.query_selector('button#Details[aria-expanded=\"false\"]')\n",
    "                    if details_button:\n",
    "                        await details_button.click()\n",
    "                        await page.wait_for_selector(\"div.accordion-body\", state=\"visible\", timeout=8000)\n",
    "                        await asyncio.sleep(1)\n",
    "                        logging.info(\"Expanded 'Details' accordion\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Could not expand Details accordion: {e}\")\n",
    "            \n",
    "                # 2. More resilient table handling\n",
    "                tables = []\n",
    "                try:\n",
    "                    tables = await page.query_selector_all(\"table.specs-table\")\n",
    "                    if not tables:\n",
    "                        await page.wait_for_selector(\"table.specs-table\", state=\"attached\", timeout=3000)\n",
    "                        tables = await page.query_selector_all(\"table.specs-table\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Table query error: {e}\")\n",
    "            \n",
    "                if not tables:\n",
    "                    product_specifications = {\"error\": \"No specifications found\"}\n",
    "                    logging.info(\"No specification tables found\")\n",
    "                else:\n",
    "                    logging.info(f\"Found {len(tables)} specification tables\")\n",
    "            \n",
    "                    for table in tables:\n",
    "                        try:\n",
    "                            # Default section title\n",
    "                            section_title = \"Specifications\"\n",
    "            \n",
    "                            # Try different header locations\n",
    "                            section_title_elem = await table.query_selector(\"thead th[colspan]\") or \\\n",
    "                                                  await table.query_selector(\"caption\") or \\\n",
    "                                                  await table.query_selector(\"thead tr:first-child th\")\n",
    "            \n",
    "                            if section_title_elem:\n",
    "                                section_title = (await section_title_elem.inner_text()).strip()\n",
    "            \n",
    "                            if section_title in seen_sections:\n",
    "                                continue\n",
    "                            seen_sections.add(section_title)\n",
    "            \n",
    "                            section_data = {}\n",
    "            \n",
    "                            rows = await table.query_selector_all(\"tbody tr\")\n",
    "                            for row in rows:\n",
    "                                try:\n",
    "                                    cells = await row.query_selector_all(\"td\")\n",
    "                                    if len(cells) >= 2:\n",
    "                                        key = (await cells[0].inner_text()).strip()\n",
    "                                        value = (await cells[1].inner_text()).strip()\n",
    "                                        if key or value:\n",
    "                                            section_data[key] = value\n",
    "                                except Exception:\n",
    "                                    continue  # Skip problematic rows\n",
    "            \n",
    "                            if section_data:\n",
    "                                product_specifications[section_title] = section_data\n",
    "            \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error processing table: {e}\")\n",
    "                            continue\n",
    "            \n",
    "                if not product_specifications:\n",
    "                    product_specifications = {\"message\": \"No specifications found\"}\n",
    "            \n",
    "            except Exception as e:\n",
    "                product_specifications = {\"error\": f\"Specifications extraction failed: {str(e)}\"}\n",
    "                logging.error(f\"Specifications extraction failed: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            # Reviews Extraction with \\n in output\n",
    "            \n",
    "            try:\n",
    "                # Expand REVIEWS accordion if collapsed\n",
    "                review_data = {\n",
    "                    \"summary\": {},\n",
    "                    \"overview\": {},\n",
    "                    \"breakdown\": {},\n",
    "                    \"reviews\": []\n",
    "                }\n",
    "                try:\n",
    "                    reviews_button = await page.query_selector('button#Reviews[aria-expanded=\"false\"]')\n",
    "                    if reviews_button:\n",
    "                        await reviews_button.click()\n",
    "                        await page.wait_for_selector(\"app-pdp-reviews-display\", state=\"visible\", timeout=8000)\n",
    "                        await asyncio.sleep(1)\n",
    "                        logging.info(\"Expanded 'Reviews' accordion successfully.\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to expand 'Reviews' accordion: {e}\")\n",
    "            \n",
    "                # Extract review summary text and overall rating\n",
    "                try:\n",
    "                    review_text = await page.locator(\".pdp-review-stars__desc\").inner_text()\n",
    "                    overall_rating = await page.locator(\".pdp-review-stars-rating__count\").inner_text()\n",
    "                    review_data[\"summary\"] = {\n",
    "                        \"text\": review_text.strip(),\n",
    "                        \"overall_rating\": overall_rating.strip()\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Review summary extraction failed: {e}\")\n",
    "                    review_data[\"summary\"] = {\n",
    "                        \"text\": \"Review summary not available\",\n",
    "                        \"overall_rating\": \"N/A\"\n",
    "                    }\n",
    "            \n",
    "                # Ratings Overview\n",
    "                try:\n",
    "                    star_rows = await page.locator(\".pdp-review-breakdown__second-ratings\").all()\n",
    "                    for row in star_rows:\n",
    "                        stars = await row.locator(\".second-ratings-name\").inner_text()\n",
    "                        count = await row.locator(\".second-rating-count\").inner_text()\n",
    "                        review_data[\"overview\"][stars.strip()] = count.strip()\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Ratings overview extraction failed: {e}\")\n",
    "            \n",
    "                # Ratings Breakdown\n",
    "                try:\n",
    "                    breakdown_rows = await page.locator(\"app-signet-pdp-rating-breakdown .pdp-review-breakdown__second-ratings\").all()\n",
    "                    for row in breakdown_rows[:3]:\n",
    "                        category = await row.locator(\".second-ratings-name\").inner_text()\n",
    "                        rating = await row.locator(\".second-rating-count\").inner_text()\n",
    "                        review_data[\"breakdown\"][category.strip()] = rating.strip()\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Ratings breakdown extraction failed: {e}\")\n",
    "            \n",
    "                # Customer Reviews\n",
    "                try:\n",
    "                    reviews = await page.locator(\".pdp-review-display__review\").all()\n",
    "                    for review in reviews[:5]:  # limit to 5\n",
    "                        try:\n",
    "                            reviewer = await review.locator(\".pdp-review-display__name\").inner_text()\n",
    "                            date = await review.locator(\".pdp-review-display__time\").inner_text()\n",
    "                            stars = len(await review.locator(\".fa-Star-Rated\").all())\n",
    "                            \n",
    "                            # Badges\n",
    "                            badges = []\n",
    "                            badge_elements = await review.locator(\".pdp-review-display__review-badge-text\").all()\n",
    "                            for badge in badge_elements:\n",
    "                                badge_text = await badge.inner_text()\n",
    "                                if badge_text.strip():\n",
    "                                    badges.append(badge_text.strip())\n",
    "            \n",
    "                            # Content\n",
    "                            title = await review.locator(\".pdp-review-display__title\").inner_text() or \"No Title\"\n",
    "                            content = await review.locator(\".pdp-review-display__content\").inner_text()\n",
    "            \n",
    "                            # Recommendation\n",
    "                            recommend = bool(await review.locator(\".pdp-review-display__recommend\").count())\n",
    "            \n",
    "                            # Helpful counts\n",
    "                            helpful_text = await review.locator(\".pdp-review-display__helpful\").inner_text()\n",
    "                            yes_match = re.search(r'Yes \\((\\d+)\\)', helpful_text)\n",
    "                            no_match = re.search(r'No \\((\\d+)\\)', helpful_text)\n",
    "                            yes_count = int(yes_match.group(1)) if yes_match else 0\n",
    "                            no_count = int(no_match.group(1)) if no_match else 0\n",
    "            \n",
    "                            # Image count\n",
    "                            images = await review.locator(\".pdp-review-display__images img\").count()\n",
    "            \n",
    "                            review_data[\"reviews\"].append({\n",
    "                                \"reviewer\": reviewer.strip(),\n",
    "                                \"date\": date.strip(),\n",
    "                                \"rating\": stars,\n",
    "                                \"badges\": badges,\n",
    "                                \"title\": title.strip(),\n",
    "                                \"content\": content.strip(),\n",
    "                                \"recommends\": recommend,\n",
    "                                \"helpful\": {\"yes\": yes_count, \"no\": no_count},\n",
    "                                \"images\": images\n",
    "                            })\n",
    "            \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error processing individual review: {e}\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Customer reviews extraction failed: {e}\")\n",
    "            \n",
    "            except Exception as review_error:\n",
    "                logging.error(f\"[Review Extraction Error] {review_error}\")\n",
    "                review_data = {\n",
    "                    \"summary\": {\"text\": \"Review information not available\", \"overall_rating\": \"N/A\"},\n",
    "                    \"overview\": {},\n",
    "                    \"breakdown\": {},\n",
    "                    \"reviews\": []\n",
    "                }\n",
    "            \n",
    "            # Convert to JSON string for DB\n",
    "            review_summary = json.dumps(review_data, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            await browser.close()\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"sku\": sku,\n",
    "                \"final_output_price\": final_output_price,\n",
    "                \"ring_sizes\": ring_sizes,\n",
    "                \"protection_plan\": protection_plan,\n",
    "                \"monthly_payment\": monthly_payment,\n",
    "                \"product_details\": product_specifications,\n",
    "                \"review_summary\": review_summary,\n",
    "                \"image_urls\": image_urls\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[Detail Error] {url}: {e}\")\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"sku\": sku,\n",
    "            \"final_output_price\": final_output_price,\n",
    "            \"ring_sizes\": ring_sizes,\n",
    "            \"protection_plan\": protection_plan,\n",
    "            \"monthly_payment\": monthly_payment,\n",
    "            \"product_details\": product_specifications,\n",
    "            \"review_summary\": review_summary,\n",
    "            \"image_urls\": image_urls\n",
    "        }\n",
    "\n",
    "        \n",
    "########################################  Main Function Call ####################################################################\n",
    "async def handle_kay(url, max_pages):\n",
    "    print(\"============================\")\n",
    "    print(url)\n",
    "    print(\"============================\")\n",
    "    # Prepare directories and files\n",
    "    os.makedirs(EXCEL_DATA_PATH, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    image_folder = os.path.join(IMAGE_SAVE_PATH, timestamp)\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    # Create workbook and setup\n",
    "    wb = Workbook()\n",
    "    sheet = wb.active\n",
    "    sheet.title = \"Products\"\n",
    "    headers = [\"Current Date\", \"Header\", \"Product Name\", \"Image\", \"Kt\", \"Price\", \"Total Dia wt\", \"Time\", \"ImagePath\", \"Additional Info\"]\n",
    "    sheet.append(headers)\n",
    "\n",
    "    all_records = []\n",
    "    records_details = []\n",
    "    filename = f\"Kay_{datetime.now().strftime('%Y-%m-%d_%H.%M')}.xlsx\"\n",
    "    file_path = os.path.join(EXCEL_DATA_PATH, filename)\n",
    "\n",
    "    page_count = 0\n",
    "    success_count = 0\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        while page_count < max_pages:\n",
    "            current_url = build_url_with_loadmore(url, page_count)\n",
    "            # logging.info(f\"Processing page {page_count}: {current_url}\")\n",
    "            browser = None\n",
    "            page = None\n",
    "            \n",
    "            try:\n",
    "                # Use the new proxy strategy function\n",
    "                #browser, page = await get_browser_with_proxy_strategy(p, current_url)\n",
    "\n",
    "                browser = await p.chromium.launch(headless=True)\n",
    "                # user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "                context = await browser.new_context()\n",
    "    \n",
    "                page = await context.new_page()\n",
    "                await page.goto(current_url, timeout=60000)\n",
    "               \n",
    "                \n",
    "                \n",
    "                log_event(f\"Successfully loaded: {current_url}\")\n",
    "                # Scroll to load all products\n",
    "                prev_product_count = 0\n",
    "                for _ in range(10):\n",
    "                    await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                    await asyncio.sleep(random.uniform(1, 2))  # Random delay between scrolls\n",
    "                    current_product_count = await page.locator('.product-item').count()\n",
    "                    if current_product_count == prev_product_count:\n",
    "                        break\n",
    "                    prev_product_count = current_product_count\n",
    "\n",
    "\n",
    "                product_wrapper = await page.query_selector(\"div.product-scroll-wrapper\")\n",
    "                products = await product_wrapper.query_selector_all(\"div.product-item\") if product_wrapper else []\n",
    "                logging.info(f\"Total products found on page {page_count}: {len(products)}\")\n",
    "\n",
    "                page_title = await page.title()\n",
    "                current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                time_only = datetime.now().strftime(\"%H.%M\")\n",
    "\n",
    "                records = []\n",
    "                image_tasks = []\n",
    "                print(\"No of products in Portal\",len(products))\n",
    "\n",
    "                for row_num, product in enumerate(products, start=len(sheet[\"A\"]) + 1):\n",
    "\n",
    "                    print(\"Record Number :\",row_num-1)\n",
    "\n",
    "                    try:\n",
    "                        base_url = \"https://www.kay.com\"\n",
    "                        product_link_element = await product.query_selector(\"a.thumb.main-thumb\")\n",
    "                        product_href = await product_link_element.get_attribute(\"href\")\n",
    "                        if product_href:\n",
    "                            product_url = base_url + product_href\n",
    "                           \n",
    "                        else:\n",
    "                            product_url = \"N/A\"\n",
    "                    except:\n",
    "                        product_url = \"N/A\"\n",
    "                    \n",
    "                    try:\n",
    "                        product_name = await (await product.query_selector(\"h2.name.product-tile-description\")).inner_text()\n",
    "                    except:\n",
    "                        product_name = \"N/A\"\n",
    "\n",
    "                    try:\n",
    "                        # Extract current price (the offer price if available)\n",
    "                        price_el = await product.query_selector(\"div.price\")\n",
    "                        current_price_text = await price_el.inner_text() if price_el else \"\"\n",
    "                        #print(f\"Current Price Text: {current_price_text}\")  # Debugging\n",
    "                        current_price = current_price_text.strip().split()[0] if current_price_text else \"\"  # ensures we get only \"$1014.30\"\n",
    "\n",
    "                        # Extract discount if available (e.g., \"30% off\")\n",
    "                        discount_el = await product.query_selector(\"span.tag-text\")\n",
    "                        discount_text = await discount_el.inner_text() if discount_el else \"\"\n",
    "                        #print(f\"Discount Text: {discount_text}\")  # Debugging\n",
    "                        discount = discount_text.replace(\" off\", \"\").strip() if discount_text else \"\"  # just \"30%\"\n",
    "\n",
    "                        # Extract original price with $ (if offer price is not available)\n",
    "                        original_price_el = await product.query_selector(\"div.original-price\")\n",
    "                        original_price_text = await original_price_el.inner_text() if original_price_el else \"\"\n",
    "                        #print(f\"Original Price Text: {original_price_text}\")  # Debugging\n",
    "                        original_price = original_price_text.strip().replace(\"Was\", \"\").strip().split()[0] if original_price_text else \"\"  # \"$1449.00\"\n",
    "\n",
    "                        # Build the final formatted price\n",
    "                        if current_price:  # If there is a current price\n",
    "                            if discount:\n",
    "                                price = f\"{current_price} offer of {discount} {original_price}\"\n",
    "                            else:\n",
    "                                price = current_price  # No discount, just current price\n",
    "                        elif original_price:  # If there is no current price but original price is available\n",
    "                            price = original_price\n",
    "                        else:\n",
    "                            price = \"N/A\"  # If neither price is available\n",
    "\n",
    "                    except Exception as e:\n",
    "                        price = \"N/A\"\n",
    "                        print(f\"Error: {e}\")  # Log the error for debugging\n",
    "\n",
    "                    try:\n",
    "                        image_url = await (await product.query_selector(\"img[itemprop='image']\")).get_attribute(\"src\")\n",
    "                    except:\n",
    "                        image_url = \"N/A\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "                    additional_info = []\n",
    "\n",
    "                    try:\n",
    "                        tag_els = await product.query_selector_all(\"span.product-tag.groupby-tablet-product-tags\")\n",
    "                        if tag_els:\n",
    "                            for tag_el in tag_els:\n",
    "                                tag_text = await tag_el.inner_text()\n",
    "                                if tag_text:\n",
    "                                    additional_info.append(tag_text.strip())\n",
    "                        else:\n",
    "                            additional_info.append(\"N/A\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        additional_info.append(\"N/A\")\n",
    "\n",
    "                    additional_info_str = \" | \".join(additional_info)    \n",
    "\n",
    "                    print(\"=================== 1st phase data =============================\")\n",
    "\n",
    "                    print(\"Row Number:\", row_num - 1)                          # int\n",
    "                    print(\"Product URL:\", product_url)                     # str\n",
    "                    print(\"Product Name:\", product_name)                   # str\n",
    "                    print(\"Price:\", price)                                 # str or float\n",
    "                    print(\"Image URL:\", image_url)                         # str or List[str]\n",
    "                    print(\"Additional Info (Raw Text or JSON):\", additional_info_str)  # str (or JSON if parsed)\n",
    "                    \n",
    "                    print(\"================================================\")\n",
    "\n",
    "\n",
    "                    detail_data = await scrape_product_detail(product_url)\n",
    "                    \n",
    "                    print(\"==================== second phase data IN ===========\")\n",
    "\n",
    "                    print(\"title:\", detail_data[\"title\"])                      # str\n",
    "                    print(\"sku:\", detail_data[\"sku\"])                          # str\n",
    "                    print(\"ring_sizes:\", detail_data[\"ring_sizes\"])            # str or List[str]\n",
    "                    print(\"final_output_price:\", detail_data[\"final_output_price\"])  # str or float\n",
    "                    print(\"protection_plan:\", detail_data[\"protection_plan\"])  # str\n",
    "                    print(\"monthly_payment:\", detail_data[\"monthly_payment\"])  # str\n",
    "                    print(\"product_details:\", detail_data[\"product_details\"])  # str (OR JSON str if you convert it)\n",
    "                    print(\"review_summary:\", detail_data[\"review_summary\"])    # str (OR JSON str if you convert it)\n",
    "                    print(\"image_urls:\", detail_data[\"image_urls\"])            # List[str] or JSON str\n",
    "                    \n",
    "                    print(\"==================== second phase data OUT ===========\")\n",
    " \n",
    "\n",
    "                    \n",
    "                    \n",
    "                    if product_name == \"N/A\" or price == \"N/A\" or image_url == \"N/A\":\n",
    "                        print(f\"Skipping product due to missing data: Name: {product_name}, Price: {price}, Image: {image_url}\")\n",
    "                        continue    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    gold_type_match = re.search(r\"\\b\\d{1,2}K\\s*(?:White|Yellow|Rose)?\\s*Gold\\b|\\bPlatinum\\b|\\bSilver\\b\", product_name, re.IGNORECASE)\n",
    "                    kt = gold_type_match.group() if gold_type_match else \"Not found\"\n",
    "\n",
    "\n",
    "                    diamond_weight_match = re.search(r\"\\d+(?:[-/]\\d+)?(?:\\s+\\d+/\\d+)?\\s*ct\\s+tw\", product_name, re.IGNORECASE)\n",
    "                    diamond_weight = diamond_weight_match.group() if diamond_weight_match else \"N/A\"\n",
    "\n",
    "\n",
    "                    unique_id = str(uuid.uuid4())\n",
    "                    image_tasks.append((row_num, unique_id, asyncio.create_task(\n",
    "                        download_image_async(image_url, product_name, timestamp, image_folder, unique_id)\n",
    "                    )))\n",
    "\n",
    "                    # portal_name =\"kay\"\n",
    "                    # product_url = product_url\n",
    "                    # SKU = detail_data[\"sku\"]\n",
    "                    # PRODUCT DESCRIPTION = detail_data[\"title\"]\n",
    "                    # PRICE =  detail_data[\"final_output_price\"]\n",
    "                    # PAYMENTPLANOPTIONS =  detail_data[\"monthly_payment\"]\n",
    "                    # AVAILABLESIZE = detail_data[\"ring_sizes\"]\n",
    "                    # PROTECTIONPLANDETAILS = detail_data[\"protection_plan\"]\n",
    "                    # PRODUCTIMAGESURLS = detail_data[\"image_urls\"]\n",
    "                    # PRODUCTSDETEAILS = detail_data[\"product_details\"]\n",
    "                    # DATEOFSCAPE = current_date\n",
    "\n",
    "                    portal_name =\"kay\"\n",
    "\n",
    "                    \n",
    "                    # Append safely\n",
    "                    records_details.append((\n",
    "                        unique_id,\n",
    "                        current_date,\n",
    "                        portal_name,\n",
    "                        page_title,\n",
    "                        detail_data.get(\"title\", \"N/A\"),\n",
    "                        detail_data.get(\"sku\", \"N/A\"),\n",
    "                        \", \".join(detail_data.get(\"ring_sizes\", [])),  # join list to string\n",
    "                        detail_data.get(\"final_output_price\", \"N/A\"),\n",
    "                        json.dumps(detail_data.get(\"protection_plan\", {})),\n",
    "                        detail_data.get(\"monthly_payment\", \"N/A\"),\n",
    "                        json.dumps(detail_data.get(\"product_details\", {})),\n",
    "                        json.dumps(detail_data.get(\"review_summary\", {})),\n",
    "                        json.dumps(detail_data.get(\"image_urls\", [])),\n",
    "                        product_url,\n",
    "                        current_date\n",
    "                    ))                                           \n",
    "\n",
    "                    records.append((unique_id, current_date, page_title, product_name, None, kt, price, diamond_weight,additional_info_str))\n",
    "                    sheet.append([current_date, page_title, product_name, None, kt, price, diamond_weight, time_only, image_url,additional_info_str])\n",
    "\n",
    "                # Process images and update records\n",
    "                for row_num, unique_id, task in image_tasks:\n",
    "                    try:\n",
    "                        image_path = await asyncio.wait_for(task, timeout=60)\n",
    "                        if image_path != \"N/A\":\n",
    "                            try:\n",
    "                                img = Image(image_path)\n",
    "                                img.width, img.height = 100, 100\n",
    "                                sheet.add_image(img, f\"D{row_num}\")\n",
    "                            except Exception as img_error:\n",
    "                                logging.error(f\"Error adding image to Excel: {img_error}\")\n",
    "                                image_path = \"N/A\"\n",
    "                        \n",
    "                        for i, record in enumerate(records):\n",
    "                            if record[0] == unique_id:\n",
    "                                records[i] = (record[0], record[1], record[2], record[3], image_path, record[5], record[6], record[7], record[8])\n",
    "                                break\n",
    "                    except asyncio.TimeoutError:\n",
    "                        logging.warning(f\"Timeout downloading image for row {row_num}\")\n",
    "\n",
    "                all_records.extend(records)\n",
    "                success_count += 1\n",
    "\n",
    "                # Save progress after each page\n",
    "                wb.save(file_path)\n",
    "                logging.info(f\"Progress saved after page {page_count}\")\n",
    "                if page:\n",
    "                    await page.close()\n",
    "                if browser:\n",
    "                    await browser.close()\n",
    "                \n",
    "                page_count += 1\n",
    "                await asyncio.sleep(random.uniform(2, 5))\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing page {page_count}: {str(e)}\")\n",
    "                if page:\n",
    "                    await page.close()\n",
    "                if browser:\n",
    "                    await browser.close()\n",
    "                wb.save(file_path)\n",
    "                continue\n",
    "            \n",
    "            # Add delay between pages\n",
    "            await asyncio.sleep(random.uniform(2, 5))\n",
    "            \n",
    "        page_count += 1\n",
    "\n",
    "    # # Final save and database operations\n",
    "    if not all_records:\n",
    "        return None, None, None\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(file_path)\n",
    "    log_event(f\"Data saved to {file_path}\")\n",
    "\n",
    "    # Encode the file in base64\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        base64_encoded = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Insert data into the database and update product count\n",
    "    insert_into_db(all_records)\n",
    "    update_product_count(len(all_records))\n",
    "    insert_into_db_details(records_details)\n",
    "\n",
    "    # Return necessary information\n",
    "    return {\"status\": \"ok\"} \n",
    "\n",
    "url = \"https://www.kay.com/engagement/c/9000000001\"\n",
    "max_pages = 1\n",
    "\n",
    "results = await handle_kay(url, max_pages)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011600d-200a-4a6c-a263-cacfea3e1cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9a401-2399-4c3c-9980-e11088a92617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "716154de-9a1f-4006-a8f3-1dfd01edefec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289bba5-5899-489d-acb6-d0dffd624ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d3ddc-c750-461c-9c5a-6d0f9a4545c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d42e0-cd69-4afd-93e7-a1d84e585c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import re\n",
    "# import time\n",
    "# import logging\n",
    "# import random\n",
    "# import uuid\n",
    "# import asyncio\n",
    "# import base64\n",
    "# from datetime import datetime\n",
    "# from playwright.async_api import async_playwright, TimeoutError, Error\n",
    "# from openpyxl import Workbook\n",
    "# from openpyxl.drawing.image import Image\n",
    "# from PIL import Image as PILImage\n",
    "# from utils import get_public_ip, log_event, sanitize_filename\n",
    "# from dotenv import load_dotenv\n",
    "# from database import insert_into_db\n",
    "# from limit_checker import update_product_count\n",
    "# from io import BytesIO\n",
    "# import httpx\n",
    "# import traceback\n",
    "# from typing import List, Tuple\n",
    "# load_dotenv()\n",
    "# PROXY_URL = os.getenv(\"PROXY_URL\")\n",
    "\n",
    "\n",
    "# PROXY_SERVER = os.getenv(\"PROXY_SERVER\")\n",
    "# PROXY_USERNAME = os.getenv(\"PROXY_USERNAME\")\n",
    "# PROXY_PASSWORD = os.getenv(\"PROXY_PASSWORD\")\n",
    "\n",
    "# EXCEL_DATA_PATH = \"./excel_data\"\n",
    "# IMAGE_SAVE_PATH = \"./images\"\n",
    "\n",
    "\n",
    "# async def download_and_resize_image(session, image_url):\n",
    "#     try:\n",
    "#         async with session.get(modify_image_url(image_url), timeout=10) as response:\n",
    "#             if response.status != 200:\n",
    "#                 return None\n",
    "#             content = await response.read()\n",
    "#             image = PILImage.open(BytesIO(content))\n",
    "#             image.thumbnail((200, 200))\n",
    "#             img_byte_arr = BytesIO()\n",
    "#             image.save(img_byte_arr, format='JPEG', optimize=True, quality=85)\n",
    "#             return img_byte_arr.getvalue()\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error downloading/resizing image: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def modify_image_url(image_url):\n",
    "#     \"\"\"Modify the image URL to replace '_260' with '_1200' while keeping query parameters.\"\"\"\n",
    "#     if not image_url or image_url == \"N/A\":\n",
    "#         return image_url\n",
    "\n",
    "#     # Extract and preserve query parameters\n",
    "#     query_params = \"\"\n",
    "#     if \"?\" in image_url:\n",
    "#         image_url, query_params = image_url.split(\"?\", 1)\n",
    "#         query_params = f\"?{query_params}\"\n",
    "\n",
    "#     # Replace '_260' with '_1200' while keeping the rest of the URL intact\n",
    "#     modified_url = re.sub(r'(_260)(?=\\.\\w+$)', '_1200', image_url)\n",
    "\n",
    "#     return modified_url + query_params  # Append query parameters if they exist\n",
    "\n",
    "# async def download_image_async(image_url, product_name, timestamp, image_folder, unique_id, retries=3):\n",
    "#     if not image_url or image_url == \"N/A\":\n",
    "#         return \"N/A\"\n",
    "\n",
    "#     image_filename = f\"{unique_id}_{timestamp}.jpg\"\n",
    "#     image_full_path = os.path.join(image_folder, image_filename)\n",
    "#     modified_url = modify_image_url(image_url)\n",
    "\n",
    "#     async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "#         for attempt in range(retries):\n",
    "#             try:\n",
    "#                 response = await client.get(modified_url)\n",
    "#                 response.raise_for_status()\n",
    "#                 with open(image_full_path, \"wb\") as f:\n",
    "#                     f.write(response.content)\n",
    "#                 return image_full_path\n",
    "#             except httpx.RequestError as e:\n",
    "#                 logging.warning(f\"Retry {attempt + 1}/{retries} - Error downloading {product_name}: {e}\")\n",
    "#     logging.error(f\"Failed to download {product_name} after {retries} attempts.\")\n",
    "#     return \"N/A\"\n",
    "\n",
    "# def random_delay(min_sec=1, max_sec=3):\n",
    "#     \"\"\"Introduce a random delay to mimic human-like behavior.\"\"\"\n",
    "#     time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "\n",
    "# ########################################  safe_goto_and_wait ####################################################################\n",
    "\n",
    "\n",
    "# async def safe_goto_and_wait(page, url,isbri_data, retries=2):\n",
    "#     for attempt in range(retries):\n",
    "#         try:\n",
    "#             print(f\"[Attempt {attempt + 1}] Navigating to: {url}\")\n",
    "            \n",
    "#             if isbri_data:\n",
    "#                 await page.goto(url, timeout=180_000, wait_until=\"domcontentloaded\")\n",
    "#             else:\n",
    "#                 await page.goto(url, wait_until=\"networkidle\", timeout=180_000)\n",
    "\n",
    "#             # Wait for the selector with a longer timeout\n",
    "#             product_cards = await page.wait_for_selector(\".product-scroll-wrapper\", state=\"attached\", timeout=30000)\n",
    "\n",
    "#             # Optionally validate at least 1 is visible (Playwright already does this)\n",
    "#             if product_cards:\n",
    "#                 print(\"[Success] Product cards loaded.\")\n",
    "#                 return\n",
    "#         except Error as e:\n",
    "#             logging.error(f\"Error navigating to {url} on attempt {attempt + 1}: {e}\")\n",
    "#             if attempt < retries - 1:\n",
    "#                 logging.info(\"Retrying after waiting a bit...\")\n",
    "#                 random_delay(1, 3)  # Add a delay before retrying\n",
    "#             else:\n",
    "#                 logging.error(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "#                 raise\n",
    "#         except TimeoutError as e:\n",
    "#             logging.warning(f\"TimeoutError on attempt {attempt + 1} navigating to {url}: {e}\")\n",
    "#             if attempt < retries - 1:\n",
    "#                 logging.info(\"Retrying after waiting a bit...\")\n",
    "#                 random_delay(1, 3)  # Add a delay before retrying\n",
    "#             else:\n",
    "#                 logging.error(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "#                 raise\n",
    "\n",
    "\n",
    "# ########################################  get browser with proxy ####################################################################\n",
    "      \n",
    "\n",
    "# async def get_browser_with_proxy_strategy(p, url: str):\n",
    "#     \"\"\"\n",
    "#     Dynamically checks robots.txt and selects proxy accordingly\n",
    "#     Always uses proxies - never scrapes directly\n",
    "#     \"\"\"\n",
    "#     parsed_url = httpx.URL(url)\n",
    "#     base_url = f\"{parsed_url.scheme}://{parsed_url.host}\"\n",
    "    \n",
    "#     # 1. Fetch and parse robots.txt\n",
    "#     disallowed_patterns = await get_robots_txt_rules(base_url)\n",
    "    \n",
    "#     # 2. Check if URL matches any disallowed pattern\n",
    "#     is_disallowed = check_url_against_rules(str(parsed_url), disallowed_patterns)\n",
    "    \n",
    "#     # 3. Try proxies in order (bri-data first if allowed, oxylabs if disallowed)\n",
    "#     proxies_to_try = [\n",
    "#         PROXY_URL if not is_disallowed else {\n",
    "#             \"server\": PROXY_SERVER,\n",
    "#             \"username\": PROXY_USERNAME,\n",
    "#             \"password\": PROXY_PASSWORD\n",
    "#         },\n",
    "#         {  # Fallback to the other proxy\n",
    "#             \"server\": PROXY_SERVER,\n",
    "#             \"username\": PROXY_USERNAME,\n",
    "#             \"password\": PROXY_PASSWORD\n",
    "#         } if not is_disallowed else PROXY_URL\n",
    "#     ]\n",
    "\n",
    "#     last_error = None\n",
    "#     for proxy_config in proxies_to_try:\n",
    "#         browser = None\n",
    "#         try:\n",
    "#             isbri_data = False\n",
    "#             if proxy_config == PROXY_URL:\n",
    "#                 logging.info(\"Attempting with bri-data proxy (allowed by robots.txt)\")\n",
    "#                 print(\"Attempting with bri-data proxy (allowed by robots.txt)\")\n",
    "#                 browser = await p.chromium.connect_over_cdp(PROXY_URL)\n",
    "#                 isbri_data = True\n",
    "#             else:\n",
    "#                 logging.info(\"Attempting with oxylabs proxy (required by robots.txt)\")\n",
    "#                 browser = await p.chromium.launch(\n",
    "#                     proxy=proxy_config,\n",
    "#                     headless=True,\n",
    "#                     args=[\n",
    "#                         '--disable-blink-features=AutomationControlled',\n",
    "#                         '--disable-web-security'\n",
    "#                     ]\n",
    "#                 )\n",
    "\n",
    "#             context = await browser.new_context()\n",
    "#             await context.add_init_script(\"\"\"\n",
    "#                 Object.defineProperty(navigator, 'webdriver', {\n",
    "#                     get: () => undefined\n",
    "#                 })\n",
    "#             \"\"\")\n",
    "#             page = await context.new_page()\n",
    "            \n",
    "#             await safe_goto_and_wait(page, url,isbri_data)\n",
    "#             return browser, page\n",
    "\n",
    "#         except Exception as e:\n",
    "#             last_error = e\n",
    "#             error_trace = traceback.format_exc()\n",
    "#             logging.error(f\"Proxy attempt failed:\\n{error_trace}\")\n",
    "#             print(f\"Proxy attempt failed:\\n{error_trace}\")\n",
    "#             if browser:\n",
    "#                 await browser.close()\n",
    "#             continue\n",
    "\n",
    "#     error_msg = (f\"Failed to load {url} using all proxy options. \"\n",
    "#                 f\"Last error: {str(last_error)}\\n\"\n",
    "#                 f\"URL may be disallowed by robots.txt or proxies failed.\")\n",
    "#     logging.error(error_msg)\n",
    "#     print(error_msg)\n",
    "#     raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# async def get_robots_txt_rules(base_url: str) -> List[str]:\n",
    "#     \"\"\"Dynamically fetch and parse robots.txt rules\"\"\"\n",
    "#     robots_url = f\"{base_url}/robots.txt\"\n",
    "#     try:\n",
    "#         async with httpx.AsyncClient() as client:\n",
    "#             resp = await client.get(robots_url, timeout=10)\n",
    "#             if resp.status_code == 200:\n",
    "#                 return [\n",
    "#                     line.split(\":\", 1)[1].strip()\n",
    "#                     for line in resp.text.splitlines()\n",
    "#                     if line.lower().startswith(\"disallow:\")\n",
    "#                 ]\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Couldn't fetch robots.txt: {e}\")\n",
    "#         print(\"Couldnt fetch robots.txt:\", {e})\n",
    "#     return []\n",
    "\n",
    "\n",
    "# def check_url_against_rules(url: str, disallowed_patterns: List[str]) -> bool:\n",
    "#     \"\"\"Check if URL matches any robots.txt disallowed pattern\"\"\"\n",
    "#     for pattern in disallowed_patterns:\n",
    "#         try:\n",
    "#             # Handle wildcard patterns\n",
    "#             if \"*\" in pattern:\n",
    "#                 regex_pattern = pattern.replace(\"*\", \".*\")\n",
    "#                 if re.search(regex_pattern, url):\n",
    "#                     return True\n",
    "#             # Handle path patterns\n",
    "#             elif url.startswith(f\"{pattern}\"):\n",
    "#                 return True\n",
    "#             # Handle query parameters\n",
    "#             elif (\"?\" in url) and any(\n",
    "#                 f\"{param}=\" in url \n",
    "#                 for param in pattern.split(\"=\")[0].split(\"*\")[-1:]\n",
    "#                 if \"=\" in pattern\n",
    "#             ):\n",
    "#                 return True\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"Error checking pattern {pattern}: {e}\")\n",
    "            \n",
    "#     return False\n",
    "\n",
    "\n",
    "\n",
    "# def build_url_with_loadmore(base_url: str, page_count: int) -> str:\n",
    "#     separator = '&' if '?' in base_url else '?'\n",
    "#     return f\"{base_url}{separator}loadMore={page_count}\"   \n",
    "\n",
    "\n",
    "\n",
    "# async def scrape_product_detail(url):\n",
    "#     title = sku = final_output_price = protection_plan = monthly_payment = result = review_summary = \"N/A\"\n",
    "#     ring_sizes = []\n",
    "#     image_urls = []\n",
    "\n",
    "#     try:\n",
    "#         async with async_playwright() as p:\n",
    "#             browser = await p.chromium.launch(headless=False)\n",
    "#             context = await browser.new_context()\n",
    "#             page = await context.new_page()\n",
    "\n",
    "#             logging.info(\"Opening page...\")\n",
    "#             await page.goto(url, timeout=60000)\n",
    "#             logging.info(f\"Successfully accessed {url}\")\n",
    "\n",
    "            \n",
    "#             # Wait for critical elements to load\n",
    "#             try:\n",
    "#                 logging.info(\"Scrolling from top to bottom to load all lazy-loaded content...\")\n",
    "#                 previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            \n",
    "#                 while True:\n",
    "#                     await page.evaluate(\"window.scrollBy(0, 1000)\")  # scroll in steps\n",
    "#                     await asyncio.sleep(0.5)  # allow lazy elements to load\n",
    "            \n",
    "#                     new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "#                     if new_height == previous_height:\n",
    "#                         break  # reached the bottom\n",
    "#                     previous_height = new_height\n",
    "            \n",
    "#                 logging.info(\"Scrolling completed.\")\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"Scrolling failed: {e}\")\n",
    "\n",
    "\n",
    "#             # SKU Extraction\n",
    "#             try:\n",
    "#                 sku_el = await page.query_selector(\".product-detail__intro--productcode\")\n",
    "#                 if sku_el:\n",
    "#                     text = (await sku_el.inner_text()).strip()\n",
    "#                     if \"Item #:\" in text:\n",
    "#                         sku = text.split(\"Item #:\")[1].strip()\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"[SKU Extraction Error] {e}\")\n",
    "\n",
    "#             # Images\n",
    "#             base_url = \"https://www.kay.com\"\n",
    "#             try:\n",
    "#                 img_elements = await page.query_selector_all(\".swiper-slide img\")\n",
    "#                 for img in img_elements:\n",
    "#                     src = await img.get_attribute(\"src\")\n",
    "#                     if src and src.startswith(\"/productimages/processed\"):\n",
    "#                         image_urls.append(base_url + src)\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"[Image Extraction Error] {e}\")\n",
    "\n",
    "#             # Title\n",
    "#             try:\n",
    "#                 title = await page.locator(\"div.product-detail__summary--name h1\").inner_text()\n",
    "#             except:\n",
    "#                 title = \"N/A\"\n",
    "\n",
    "\n",
    "#             # Prices and Discount\n",
    "#             try:\n",
    "#                 discounted_price = await page.locator(\"span.product-price__price\").inner_text()\n",
    "#             except:\n",
    "#                 discounted_price = \"N/A\"\n",
    "            \n",
    "#             try:\n",
    "#                 original_price = await page.locator(\"span.product-price__striked\").inner_text()\n",
    "#             except:\n",
    "#                 original_price = None\n",
    "            \n",
    "#             try:\n",
    "#                 discount = await page.locator(\"span.tag-text\").inner_text()\n",
    "#             except:\n",
    "#                 discount = None\n",
    "            \n",
    "#             # Construct final string smartly\n",
    "#             price_info = f\"Discounted Price: {discounted_price.strip()}\"\n",
    "            \n",
    "#             price_info += f\" | Original Price: {original_price.strip() if original_price else 'N/A'}\"\n",
    "#             price_info += f\" | Discount: {discount.strip() if discount else 'N/A'}\"\n",
    "            \n",
    "#             final_output_price = price_info\n",
    "\n",
    "#             # Monthly Payment Full Sentence Extraction (Cleaned)\n",
    "#             try:\n",
    "#                 element = page.locator(\"div.ep-text-placement\")\n",
    "#                 monthly_payment = await element.inner_text()\n",
    "#                 if monthly_payment:\n",
    "#                     # Clean newlines and superscript artifacts like ^, footnotes, etc.\n",
    "#                     monthly_payment = monthly_payment.replace(\"\\n\", \" \").strip()\n",
    "#                     monthly_payment = monthly_payment.replace(\" ^\", \"\").replace(\"^\", \"\")\n",
    "#                 else:\n",
    "#                     monthly_payment = \"N/A\"\n",
    "#             except Exception:\n",
    "#                 monthly_payment = \"N/A\"\n",
    "\n",
    "\n",
    "#             # Ring Sizes\n",
    "#             try:\n",
    "#                 size_elements = await page.locator(\"div.ring-size-selector label.selector-label\").all_inner_texts()\n",
    "#                 ring_sizes = [s.strip().replace(\"*\", \"\") for s in size_elements]\n",
    "#             except:\n",
    "#                 ring_sizes = []\n",
    "\n",
    "            \n",
    "#             # Protection Plan\n",
    "#             try:\n",
    "#                 # Extract header title and subtitle\n",
    "#                 title_block = await page.locator(\"div.warranty_heading span\").all_inner_texts()\n",
    "#                 main_title = title_block[0].strip() if len(title_block) > 0 else \"\"\n",
    "#                 subtitle = title_block[1].strip() if len(title_block) > 1 else \"\"\n",
    "#                 header = f\"{main_title} \\\\n{subtitle} \\\\n\\\\n\"\n",
    "            \n",
    "#                 # Extract service rows\n",
    "#                 rows = await page.locator(\"table.table tbody tr\").all()\n",
    "#                 services = []\n",
    "#                 for row in rows:\n",
    "#                     cols = await row.locator(\"td\").all_inner_texts()\n",
    "#                     if len(cols) >= 3:\n",
    "#                         service = (\n",
    "#                             f\"Service Needed: {cols[0].strip()} \\\\n\"\n",
    "#                             f\"Typical Cost: {cols[1].strip()} \\\\n\"\n",
    "#                             f\"With This Plan: {cols[2].strip()} \\\\n\\\\n\"\n",
    "#                         )\n",
    "#                         services.append(service)\n",
    "            \n",
    "#                 # Join everything into one string\n",
    "#                 protection_plan = header + \"\".join(services).strip()\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 protection_plan = f\"Error extracting protection plan: {e}\"\n",
    "\n",
    "          \n",
    "#             #Product Specifications\n",
    "#             try:\n",
    "#                 result = \"\"\n",
    "#                 seen_sections = set()\n",
    "            \n",
    "#                 # 1. Expand 'Details' accordion if it's collapsed\n",
    "#                 try:\n",
    "#                     details_button = await page.query_selector('button#Details[aria-expanded=\"false\"]')\n",
    "#                     if details_button:\n",
    "#                         await details_button.click()\n",
    "#                         await page.wait_for_selector(\"div.accordion-body\", state=\"visible\", timeout=8000)\n",
    "#                         await asyncio.sleep(2)\n",
    "#                         logging.info(\"Expanded 'Details' accordion\")\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Could not expand Details accordion: {e}\")\n",
    "            \n",
    "#                 # 2. Look for all specs tables\n",
    "#                 await page.wait_for_selector(\"table.specs-table\", timeout=10000)\n",
    "#                 tables = await page.query_selector_all(\"table.specs-table\")\n",
    "            \n",
    "#                 if not tables:\n",
    "#                     result = \"No specifications found\"\n",
    "#                 else:\n",
    "#                     for table in tables:\n",
    "#                         # Section header like \"Stone(s)\", \"Metal(s)\", etc.\n",
    "#                         section_title_elem = await table.query_selector(\"thead th[colspan]\")\n",
    "#                         section_title = (await section_title_elem.inner_text()).strip() if section_title_elem else \"Specifications\"\n",
    "            \n",
    "#                         if section_title in seen_sections:\n",
    "#                             continue\n",
    "#                         seen_sections.add(section_title)\n",
    "            \n",
    "#                         result += f\"{section_title}:\\n\"\n",
    "            \n",
    "#                         # Key-value pairs\n",
    "#                         rows = await table.query_selector_all(\"tbody tr\")\n",
    "#                         for row in rows:\n",
    "#                             cells = await row.query_selector_all(\"td\")\n",
    "#                             if len(cells) >= 2:\n",
    "#                                 key = (await cells[0].inner_text()).strip()\n",
    "#                                 value = (await cells[1].inner_text()).strip()\n",
    "#                                 result += f\"{key}: {value}\\n\"\n",
    "            \n",
    "#                         result += \"\\n\"\n",
    "            \n",
    "#                 if not result.strip():\n",
    "#                     result = \"No specifications found\"\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 result = f\"Specifications error: {str(e)}\"\n",
    "#                 logging.error(f\"Specifications extraction failed: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # Reviews Extraction with \\n in output\n",
    "#             try:\n",
    "#                 # Expand Reviews accordion if collapsed\n",
    "#                 reviews_button = await page.query_selector('button#Reviews[aria-expanded=\"false\"]')\n",
    "#                 if reviews_button:\n",
    "#                     await reviews_button.click()\n",
    "#                     await page.wait_for_selector(\"div.accordion-body\", state=\"visible\", timeout=8000)\n",
    "#                     await asyncio.sleep(1)\n",
    "#                     logging.info(\"Expanded 'Reviews' accordion\")\n",
    "                \n",
    "#                 # Extract review summary\n",
    "#                 review_text = await page.locator(\".pdp-review-stars__desc\").inner_text()\n",
    "#                 overall_rating = await page.locator(\".pdp-review-stars-rating__count\").inner_text()\n",
    "#                 write_review = await page.locator(\"#writeAReview .btn-txt-content\").inner_text()\n",
    "                \n",
    "#                 review_summary = (\n",
    "#                     f\"{review_text.strip()}\\n\"\n",
    "#                     f\"Overall Rating: {overall_rating.strip()}\\n\"\n",
    "#                     f\"{write_review.strip()}\\n\\n\"\n",
    "#                 )\n",
    "                \n",
    "#                 # Extract ratings overview\n",
    "#                 review_summary += \"Ratings Overview:\\n\"\n",
    "#                 star_rows = await page.query_selector_all(\".pdp-review-breakdown__second-ratings\")\n",
    "#                 for row in star_rows:\n",
    "#                     stars = await row.locator(\".second-ratings-name\").inner_text()\n",
    "#                     count = await row.locator(\".second-rating-count\").inner_text()\n",
    "#                     review_summary += f\"{stars}: {count}\\n\"\n",
    "                \n",
    "#                 # Extract ratings breakdown\n",
    "#                 review_summary += \"\\nRatings Breakdown:\\n\"\n",
    "#                 breakdown_rows = await page.query_selector_all(\".pdp-review-breakdown__second-ratings\")\n",
    "#                 for i in range(3):  # First 3 are Quality, Value, Appearance\n",
    "#                     category = await breakdown_rows[i].locator(\".second-ratings-name\").inner_text()\n",
    "#                     rating = await breakdown_rows[i].locator(\".second-rating-count\").inner_text()\n",
    "#                     review_summary += f\"{category}: {rating}\\n\"\n",
    "                \n",
    "#                 # Extract individual reviews\n",
    "#                 review_summary += \"\\nCustomer Reviews:\\n\"\n",
    "#                 reviews = await page.query_selector_all(\".pdp-review-display__review\")\n",
    "#                 review_count = min(len(reviews), 5)  # Limit to first 5 reviews\n",
    "                \n",
    "#                 for i in range(review_count):\n",
    "#                     review = reviews[i]\n",
    "                    \n",
    "#                     # Basic info\n",
    "#                     reviewer = await review.locator(\".pdp-review-display__name\").inner_text()\n",
    "#                     date = await review.locator(\".pdp-review-display__time\").inner_text()\n",
    "#                     stars = len(await review.locator(\".fa-Star-Rated\").all())\n",
    "                    \n",
    "#                     # Badges\n",
    "#                     badges = []\n",
    "#                     badge_elements = await review.query_selector_all(\".pdp-review-display__review-badge-text\")\n",
    "#                     for badge in badge_elements:\n",
    "#                         badges.append(await badge.inner_text())\n",
    "                    \n",
    "#                     # Content\n",
    "#                     title = await review.locator(\".pdp-review-display__title\").inner_text() or \"No Title\"\n",
    "#                     content = await review.locator(\".pdp-review-display__content\").inner_text()\n",
    "                    \n",
    "#                     # Recommendation\n",
    "#                     recommend = \"Yes\" if await review.query_selector(\".pdp-review-display__recommend\") else \"No\"\n",
    "                    \n",
    "#                     # Helpful counts\n",
    "#                     helpful_text = await review.locator(\".pdp-review-display__helpful\").inner_text()\n",
    "#                     yes_count = re.search(r'Yes \\((\\d+)\\)', helpful_text).group(1) if re.search(r'Yes \\(\\d+\\)', helpful_text) else \"0\"\n",
    "#                     no_count = re.search(r'No \\((\\d+)\\)', helpful_text).group(1) if re.search(r'No \\(\\d+\\)', helpful_text) else \"0\"\n",
    "                    \n",
    "#                     # Images\n",
    "#                     images = await review.locator(\".pdp-review-display__images img\").count()\n",
    "                    \n",
    "#                     # Format review entry\n",
    "#                     review_summary += (\n",
    "#                         f\"\\nReview {i+1}:\\n\"\n",
    "#                         f\"Reviewer: {reviewer}\\n\"\n",
    "#                         f\"Date: {date}\\n\"\n",
    "#                         f\"Rating: {'' * stars}\\n\"\n",
    "#                         f\"Badges: {', '.join(badges)}\\n\"\n",
    "#                         f\"Title: {title}\\n\"\n",
    "#                         f\"Content: {content}\\n\"\n",
    "#                         f\"Recommends: {recommend}\\n\"\n",
    "#                         f\"Helpful: Yes({yes_count}), No({no_count})\\n\"\n",
    "#                         f\"Images: {images}\\n\"\n",
    "#                         f\"{'-'*40}\"\n",
    "#                     )\n",
    "            \n",
    "#             except Exception as review_error:\n",
    "#                 logging.warning(f\"[Review Extraction Error] {review_error}\")\n",
    "#                 review_summary = \"Review information not available.\"\n",
    "                     \n",
    "\n",
    "\n",
    "\n",
    "#             await browser.close()\n",
    "\n",
    "#             return {\n",
    "#                 \"title\": title,\n",
    "#                 \"sku\": sku,\n",
    "#                 \"final_output_price\": final_output_price,\n",
    "#                 \"ring_sizes\": ring_sizes,\n",
    "#                 \"protection_plan\": protection_plan,\n",
    "#                 \"monthly_payment\": monthly_payment,\n",
    "#                 \"product_details\": result,\n",
    "#                 \"review_summary\": review_summary,\n",
    "#                 \"image_urls\": image_urls\n",
    "#             }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"[Detail Error] {url}: {e}\")\n",
    "#         return {\n",
    "#             \"title\": title,\n",
    "#             \"sku\": sku,\n",
    "#             \"final_output_price\": final_output_price,\n",
    "#             \"ring_sizes\": ring_sizes,\n",
    "#             \"protection_plan\": protection_plan,\n",
    "#             \"monthly_payment\": monthly_payment,\n",
    "#             \"product_details\": result,\n",
    "#             \"review_summary\": review_summary,\n",
    "#             \"image_urls\": image_urls\n",
    "#         }\n",
    "         \n",
    "# ########################################  Main Function Call ####################################################################\n",
    "# async def handle_kay(url, max_pages):\n",
    "#     print(\"============================\")\n",
    "#     print(url)\n",
    "#     print(\"============================\")\n",
    "#     # Prepare directories and files\n",
    "#     os.makedirs(EXCEL_DATA_PATH, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     image_folder = os.path.join(IMAGE_SAVE_PATH, timestamp)\n",
    "#     os.makedirs(image_folder, exist_ok=True)\n",
    "#     # Create workbook and setup\n",
    "#     wb = Workbook()\n",
    "#     sheet = wb.active\n",
    "#     sheet.title = \"Products\"\n",
    "#     headers = [\"Current Date\", \"Header\", \"Product Name\", \"Image\", \"Kt\", \"Price\", \"Total Dia wt\", \"Time\", \"ImagePath\", \"Additional Info\"]\n",
    "#     sheet.append(headers)\n",
    "\n",
    "#     all_records = []\n",
    "#     filename = f\"Kay_{datetime.now().strftime('%Y-%m-%d_%H.%M')}.xlsx\"\n",
    "#     file_path = os.path.join(EXCEL_DATA_PATH, filename)\n",
    "\n",
    "#     page_count = 0\n",
    "#     success_count = 0\n",
    "\n",
    "#     async with async_playwright() as p:\n",
    "#         while page_count < max_pages:\n",
    "#             current_url = build_url_with_loadmore(url, page_count)\n",
    "#             # logging.info(f\"Processing page {page_count}: {current_url}\")\n",
    "#             browser = None\n",
    "#             page = None\n",
    "            \n",
    "#             try:\n",
    "#                 # Use the new proxy strategy function\n",
    "#                 browser, page = await get_browser_with_proxy_strategy(p, current_url)\n",
    "#                 log_event(f\"Successfully loaded: {current_url}\")\n",
    "#                 # Scroll to load all products\n",
    "#                 prev_product_count = 0\n",
    "#                 for _ in range(10):\n",
    "#                     await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "#                     await asyncio.sleep(random.uniform(1, 2))  # Random delay between scrolls\n",
    "#                     current_product_count = await page.locator('.product-item').count()\n",
    "#                     if current_product_count == prev_product_count:\n",
    "#                         break\n",
    "#                     prev_product_count = current_product_count\n",
    "\n",
    "\n",
    "#                 product_wrapper = await page.query_selector(\"div.product-scroll-wrapper\")\n",
    "#                 products = await product_wrapper.query_selector_all(\"div.product-item\") if product_wrapper else []\n",
    "#                 logging.info(f\"Total products found on page {page_count}: {len(products)}\")\n",
    "\n",
    "#                 page_title = await page.title()\n",
    "#                 current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#                 time_only = datetime.now().strftime(\"%H.%M\")\n",
    "\n",
    "#                 records = []\n",
    "#                 image_tasks = []\n",
    "\n",
    "#                 for row_num, product in enumerate(products, start=len(sheet[\"A\"]) + 1):\n",
    "\n",
    "#                     base_url = \"https://www.kay.com\"\n",
    "\n",
    "#                     try:\n",
    "#                         product_link_element = await product.query_selector(\"a.thumb.main-thumb\")\n",
    "#                         product_href = await product_link_element.get_attribute(\"href\")\n",
    "#                         if product_href:\n",
    "#                             product_url = base_url + product_href\n",
    "                           \n",
    "#                         else:\n",
    "#                             product_url = \"N/A\"\n",
    "#                     except:\n",
    "#                         product_url = \"N/A\"\n",
    "                    \n",
    "#                     try:\n",
    "#                         product_name = await (await product.query_selector(\"h2.name.product-tile-description\")).inner_text()\n",
    "#                     except:\n",
    "#                         product_name = \"N/A\"\n",
    "\n",
    "#                     try:\n",
    "#                         # Extract current price (the offer price if available)\n",
    "#                         price_el = await product.query_selector(\"div.price\")\n",
    "#                         current_price_text = await price_el.inner_text() if price_el else \"\"\n",
    "#                         #print(f\"Current Price Text: {current_price_text}\")  # Debugging\n",
    "#                         current_price = current_price_text.strip().split()[0] if current_price_text else \"\"  # ensures we get only \"$1014.30\"\n",
    "\n",
    "#                         # Extract discount if available (e.g., \"30% off\")\n",
    "#                         discount_el = await product.query_selector(\"span.tag-text\")\n",
    "#                         discount_text = await discount_el.inner_text() if discount_el else \"\"\n",
    "#                         #print(f\"Discount Text: {discount_text}\")  # Debugging\n",
    "#                         discount = discount_text.replace(\" off\", \"\").strip() if discount_text else \"\"  # just \"30%\"\n",
    "\n",
    "#                         # Extract original price with $ (if offer price is not available)\n",
    "#                         original_price_el = await product.query_selector(\"div.original-price\")\n",
    "#                         original_price_text = await original_price_el.inner_text() if original_price_el else \"\"\n",
    "#                         #print(f\"Original Price Text: {original_price_text}\")  # Debugging\n",
    "#                         original_price = original_price_text.strip().replace(\"Was\", \"\").strip().split()[0] if original_price_text else \"\"  # \"$1449.00\"\n",
    "\n",
    "#                         # Build the final formatted price\n",
    "#                         if current_price:  # If there is a current price\n",
    "#                             if discount:\n",
    "#                                 price = f\"{current_price} offer of {discount} {original_price}\"\n",
    "#                             else:\n",
    "#                                 price = current_price  # No discount, just current price\n",
    "#                         elif original_price:  # If there is no current price but original price is available\n",
    "#                             price = original_price\n",
    "#                         else:\n",
    "#                             price = \"N/A\"  # If neither price is available\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         price = \"N/A\"\n",
    "#                         print(f\"Error: {e}\")  # Log the error for debugging\n",
    "\n",
    "#                     try:\n",
    "#                         image_url = await (await product.query_selector(\"img[itemprop='image']\")).get_attribute(\"src\")\n",
    "#                     except:\n",
    "#                         image_url = \"N/A\"\n",
    "\n",
    "#                     detail_data = await scrape_product_detail(product_url)\n",
    "#                     print(\"==================== second phase data IN ===========\")\n",
    "#                     print(\"title\",detail_data[\"title\"])\n",
    "#                     print(\"sku\",detail_data[\"sku\"])\n",
    "#                     print(\"ring_sizes\",detail_data[\"ring_sizes\"])\n",
    "#                     print(\"final_output_price\",detail_data[\"final_output_price\"])\n",
    "#                     print(\"protection_plan\",detail_data[\"protection_plan\"])\n",
    "#                     print(\"monthly_payment\",detail_data[\"monthly_payment\"])\n",
    "#                     print(\"product_details\",detail_data[\"product_details\"])\n",
    "#                     print(\"review_summary\",detail_data[\"review_summary\"])\n",
    "#                     print(\"image_urls\",detail_data[\"image_urls\"])\n",
    "#                     print(\"====================second phase data  OUT ===========\")    \n",
    "                        \n",
    "#                     additional_info = []\n",
    "\n",
    "#                     try:\n",
    "#                         tag_els = await product.query_selector_all(\"span.product-tag.groupby-tablet-product-tags\")\n",
    "#                         if tag_els:\n",
    "#                             for tag_el in tag_els:\n",
    "#                                 tag_text = await tag_el.inner_text()\n",
    "#                                 if tag_text:\n",
    "#                                     additional_info.append(tag_text.strip())\n",
    "#                         else:\n",
    "#                             additional_info.append(\"N/A\")\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         additional_info.append(\"N/A\")\n",
    "\n",
    "#                     additional_info_str = \" | \".join(additional_info)    \n",
    "\n",
    "#                     print(\"=================== 1st phase data =============================\")\n",
    "#                     print(row_num)\n",
    "#                     print(product_url)\n",
    "#                     print(product_name)\n",
    "#                     print(price)\n",
    "#                     print(image_url)\n",
    "#                     print(additional_info_str)\n",
    "#                     print(\"================================================\")\n",
    "                    \n",
    "                    \n",
    "#                     if product_name == \"N/A\" or price == \"N/A\" or image_url == \"N/A\":\n",
    "#                         print(f\"Skipping product due to missing data: Name: {product_name}, Price: {price}, Image: {image_url}\")\n",
    "#                         continue    \n",
    "                    \n",
    "                    \n",
    "\n",
    "#                     gold_type_match = re.search(r\"\\b\\d{1,2}K\\s*(?:White|Yellow|Rose)?\\s*Gold\\b|\\bPlatinum\\b|\\bSilver\\b\", product_name, re.IGNORECASE)\n",
    "#                     kt = gold_type_match.group() if gold_type_match else \"Not found\"\n",
    "\n",
    "\n",
    "#                     diamond_weight_match = re.search(r\"\\d+(?:[-/]\\d+)?(?:\\s+\\d+/\\d+)?\\s*ct\\s+tw\", product_name, re.IGNORECASE)\n",
    "#                     diamond_weight = diamond_weight_match.group() if diamond_weight_match else \"N/A\"\n",
    "\n",
    "\n",
    "#                     unique_id = str(uuid.uuid4())\n",
    "#                     image_tasks.append((row_num, unique_id, asyncio.create_task(\n",
    "#                         download_image_async(image_url, product_name, timestamp, image_folder, unique_id)\n",
    "#                     )))\n",
    "\n",
    "#                     records.append((unique_id, current_date, page_title, product_name, None, kt, price, diamond_weight,additional_info_str))\n",
    "#                     sheet.append([current_date, page_title, product_name, None, kt, price, diamond_weight, time_only, image_url,additional_info_str])\n",
    "\n",
    "#                 # Process images and update records\n",
    "#                 for row_num, unique_id, task in image_tasks:\n",
    "#                     try:\n",
    "#                         image_path = await asyncio.wait_for(task, timeout=60)\n",
    "#                         if image_path != \"N/A\":\n",
    "#                             try:\n",
    "#                                 img = Image(image_path)\n",
    "#                                 img.width, img.height = 100, 100\n",
    "#                                 sheet.add_image(img, f\"D{row_num}\")\n",
    "#                             except Exception as img_error:\n",
    "#                                 logging.error(f\"Error adding image to Excel: {img_error}\")\n",
    "#                                 image_path = \"N/A\"\n",
    "                        \n",
    "#                         for i, record in enumerate(records):\n",
    "#                             if record[0] == unique_id:\n",
    "#                                 records[i] = (record[0], record[1], record[2], record[3], image_path, record[5], record[6], record[7], record[8])\n",
    "#                                 break\n",
    "#                     except asyncio.TimeoutError:\n",
    "#                         logging.warning(f\"Timeout downloading image for row {row_num}\")\n",
    "\n",
    "#                 all_records.extend(records)\n",
    "#                 success_count += 1\n",
    "\n",
    "#                 # Save progress after each page\n",
    "#                 wb.save(file_path)\n",
    "#                 logging.info(f\"Progress saved after page {page_count}\")\n",
    "#                 if page:\n",
    "#                     await page.close()\n",
    "#                 if browser:\n",
    "#                     await browser.close()\n",
    "                \n",
    "#                 page_count += 1\n",
    "#                 await asyncio.sleep(random.uniform(2, 5))\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error processing page {page_count}: {str(e)}\")\n",
    "#                 if page:\n",
    "#                     await page.close()\n",
    "#                 if browser:\n",
    "#                     await browser.close()\n",
    "#                 wb.save(file_path)\n",
    "#                 continue\n",
    "            \n",
    "#             # Add delay between pages\n",
    "#             await asyncio.sleep(random.uniform(2, 5))\n",
    "            \n",
    "#         page_count += 1\n",
    "\n",
    "#     # # Final save and database operations\n",
    "#     if not all_records:\n",
    "#         return None, None, None\n",
    "\n",
    "#     # Save the workbook\n",
    "#     wb.save(file_path)\n",
    "#     log_event(f\"Data saved to {file_path}\")\n",
    "\n",
    "#     # Encode the file in base64\n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         base64_encoded = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "#     # Insert data into the database and update product count\n",
    "#     insert_into_db(all_records)\n",
    "#     update_product_count(len(all_records))\n",
    "\n",
    "#     # Return necessary information\n",
    "#     return {\"status\": \"ok\"} \n",
    "\n",
    "# url = \"https://www.kay.com/engagement/c/9000000001?query=:_relevance_Ascending:v_stoneTypes_uFilter:%2522Lab-Grown%20Diamond%2522:v_gender_uFilter:%2522WOMEN%2522&unbxdAPI=true\"\n",
    "# max_pages = 1\n",
    "\n",
    "# results = await handle_kay(url, max_pages)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a14d5-c721-4e81-bb09-ab03f11fdc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b32ef-f5ce-41de-bcdc-31eb4a5d9d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc0827-9c02-4cf6-8e48-b86649ceff98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d4825-eb54-40af-8e7b-c96214bdf7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ba653-998f-4b48-8d6f-e65af57025c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c9d29-35c1-47f1-a8d4-9df66ec3d880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ede49-b9ca-46b7-9e52-78dec09e36ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f89783-bdfe-47f6-a2dc-ff772822a9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df878718-8af6-476b-928c-9ef0df7714c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdf875-4f41-4023-9790-a50b12e3019b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bc5e7-2441-4877-9894-1b2153c5e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a5825-3634-4555-a4b0-52f594c2f6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072e3e6-1af1-4991-a4e5-56a1924fa7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4628333-559e-47ce-bef0-0a2131df217b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687a952-6b65-4e25-8cd6-489ce768fee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360616d-cfb2-43f7-bc0e-f0207351661e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb550a-57a3-4289-8ed9-528713dd8b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7bf0c-2a4d-4e79-a85d-9781e4cf6286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673b903-2508-4161-a0ee-d3fdfd52cae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd2bd1-e0a1-4cc4-8338-155cfcc8b1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076117c-7e90-4e9e-ba27-1e4d9b000890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae74f8-3506-4a39-8f71-5a3e4d45ab2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a4db2-2021-4662-a9e6-eee13f749537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff970aaa-f303-4e98-8529-a773c61d114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821594ba-9951-47ae-abb8-d2be76a85fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331d06e-bbe3-4659-9512-413984ac6c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e888ea7-158d-4750-a23a-ad6067acd491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8534c6-0126-4b67-84a9-7f3f875940cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60281fa-7a6f-4bf2-8247-96a5bb93c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd38c3-dea5-4daf-947a-b9e7c6cc2254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676808a-64b1-4903-bf80-778735a7fddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075fe83-208d-4637-aae6-400d67f6b345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7601-5b64-4893-b256-3e4dad3cd03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183a33b-df38-41e8-9279-8cb02a0c45cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a283775-43df-486e-8921-ee0b3934db9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a29b5-3cb0-4d21-a087-599f3a09d252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0e78f-1c5e-4f1a-a6fd-33116a934f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import re\n",
    "# import time\n",
    "# import logging\n",
    "# import random\n",
    "# import uuid\n",
    "# import asyncio\n",
    "# import base64\n",
    "# from datetime import datetime\n",
    "# from playwright.async_api import async_playwright, TimeoutError, Error\n",
    "# from openpyxl import Workbook\n",
    "# from openpyxl.drawing.image import Image\n",
    "# from PIL import Image as PILImage\n",
    "# from utils import get_public_ip, log_event, sanitize_filename\n",
    "# from dotenv import load_dotenv\n",
    "# from database import insert_into_db\n",
    "# from limit_checker import update_product_count\n",
    "# from io import BytesIO\n",
    "# import httpx\n",
    "# import traceback\n",
    "# from typing import List, Tuple\n",
    "# load_dotenv()\n",
    "# PROXY_URL = os.getenv(\"PROXY_URL\")\n",
    "\n",
    "\n",
    "# PROXY_SERVER = os.getenv(\"PROXY_SERVER\")\n",
    "# PROXY_USERNAME = os.getenv(\"PROXY_USERNAME\")\n",
    "# PROXY_PASSWORD = os.getenv(\"PROXY_PASSWORD\")\n",
    "\n",
    "# EXCEL_DATA_PATH = \"./excel_data\"\n",
    "# IMAGE_SAVE_PATH = \"./images\"\n",
    "\n",
    "\n",
    "# async def download_and_resize_image(session, image_url):\n",
    "#     try:\n",
    "#         async with session.get(modify_image_url(image_url), timeout=10) as response:\n",
    "#             if response.status != 200:\n",
    "#                 return None\n",
    "#             content = await response.read()\n",
    "#             image = PILImage.open(BytesIO(content))\n",
    "#             image.thumbnail((200, 200))\n",
    "#             img_byte_arr = BytesIO()\n",
    "#             image.save(img_byte_arr, format='JPEG', optimize=True, quality=85)\n",
    "#             return img_byte_arr.getvalue()\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Error downloading/resizing image: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def modify_image_url(image_url):\n",
    "#     \"\"\"Modify the image URL to replace '_260' with '_1200' while keeping query parameters.\"\"\"\n",
    "#     if not image_url or image_url == \"N/A\":\n",
    "#         return image_url\n",
    "\n",
    "#     # Extract and preserve query parameters\n",
    "#     query_params = \"\"\n",
    "#     if \"?\" in image_url:\n",
    "#         image_url, query_params = image_url.split(\"?\", 1)\n",
    "#         query_params = f\"?{query_params}\"\n",
    "\n",
    "#     # Replace '_260' with '_1200' while keeping the rest of the URL intact\n",
    "#     modified_url = re.sub(r'(_260)(?=\\.\\w+$)', '_1200', image_url)\n",
    "\n",
    "#     return modified_url + query_params  # Append query parameters if they exist\n",
    "\n",
    "# async def download_image_async(image_url, product_name, timestamp, image_folder, unique_id, retries=3):\n",
    "#     if not image_url or image_url == \"N/A\":\n",
    "#         return \"N/A\"\n",
    "\n",
    "#     image_filename = f\"{unique_id}_{timestamp}.jpg\"\n",
    "#     image_full_path = os.path.join(image_folder, image_filename)\n",
    "#     modified_url = modify_image_url(image_url)\n",
    "\n",
    "#     async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "#         for attempt in range(retries):\n",
    "#             try:\n",
    "#                 response = await client.get(modified_url)\n",
    "#                 response.raise_for_status()\n",
    "#                 with open(image_full_path, \"wb\") as f:\n",
    "#                     f.write(response.content)\n",
    "#                 return image_full_path\n",
    "#             except httpx.RequestError as e:\n",
    "#                 logging.warning(f\"Retry {attempt + 1}/{retries} - Error downloading {product_name}: {e}\")\n",
    "#     logging.error(f\"Failed to download {product_name} after {retries} attempts.\")\n",
    "#     return \"N/A\"\n",
    "\n",
    "# def random_delay(min_sec=1, max_sec=3):\n",
    "#     \"\"\"Introduce a random delay to mimic human-like behavior.\"\"\"\n",
    "#     time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "\n",
    "# ########################################  safe_goto_and_wait ####################################################################\n",
    "\n",
    "\n",
    "# async def safe_goto_and_wait(page, url,isbri_data, retries=2):\n",
    "#     for attempt in range(retries):\n",
    "#         try:\n",
    "#             print(f\"[Attempt {attempt + 1}] Navigating to: {url}\")\n",
    "            \n",
    "#             if isbri_data:\n",
    "#                 await page.goto(url, timeout=180_000, wait_until=\"domcontentloaded\")\n",
    "#             else:\n",
    "#                 await page.goto(url, wait_until=\"networkidle\", timeout=180_000)\n",
    "\n",
    "#             # Wait for the selector with a longer timeout\n",
    "#             product_cards = await page.wait_for_selector(\".product-scroll-wrapper\", state=\"attached\", timeout=30000)\n",
    "\n",
    "#             # Optionally validate at least 1 is visible (Playwright already does this)\n",
    "#             if product_cards:\n",
    "#                 print(\"[Success] Product cards loaded.\")\n",
    "#                 return\n",
    "#         except Error as e:\n",
    "#             logging.error(f\"Error navigating to {url} on attempt {attempt + 1}: {e}\")\n",
    "#             if attempt < retries - 1:\n",
    "#                 logging.info(\"Retrying after waiting a bit...\")\n",
    "#                 random_delay(1, 3)  # Add a delay before retrying\n",
    "#             else:\n",
    "#                 logging.error(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "#                 raise\n",
    "#         except TimeoutError as e:\n",
    "#             logging.warning(f\"TimeoutError on attempt {attempt + 1} navigating to {url}: {e}\")\n",
    "#             if attempt < retries - 1:\n",
    "#                 logging.info(\"Retrying after waiting a bit...\")\n",
    "#                 random_delay(1, 3)  # Add a delay before retrying\n",
    "#             else:\n",
    "#                 logging.error(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "#                 raise\n",
    "\n",
    "\n",
    "# ########################################  get browser with proxy ####################################################################\n",
    "      \n",
    "\n",
    "# async def get_browser_with_proxy_strategy(p, url: str):\n",
    "#     \"\"\"\n",
    "#     Dynamically checks robots.txt and selects proxy accordingly\n",
    "#     Always uses proxies - never scrapes directly\n",
    "#     \"\"\"\n",
    "#     parsed_url = httpx.URL(url)\n",
    "#     base_url = f\"{parsed_url.scheme}://{parsed_url.host}\"\n",
    "    \n",
    "#     # 1. Fetch and parse robots.txt\n",
    "#     disallowed_patterns = await get_robots_txt_rules(base_url)\n",
    "    \n",
    "#     # 2. Check if URL matches any disallowed pattern\n",
    "#     is_disallowed = check_url_against_rules(str(parsed_url), disallowed_patterns)\n",
    "    \n",
    "#     # 3. Try proxies in order (bri-data first if allowed, oxylabs if disallowed)\n",
    "#     proxies_to_try = [\n",
    "#         PROXY_URL if not is_disallowed else {\n",
    "#             \"server\": PROXY_SERVER,\n",
    "#             \"username\": PROXY_USERNAME,\n",
    "#             \"password\": PROXY_PASSWORD\n",
    "#         },\n",
    "#         {  # Fallback to the other proxy\n",
    "#             \"server\": PROXY_SERVER,\n",
    "#             \"username\": PROXY_USERNAME,\n",
    "#             \"password\": PROXY_PASSWORD\n",
    "#         } if not is_disallowed else PROXY_URL\n",
    "#     ]\n",
    "\n",
    "#     last_error = None\n",
    "#     for proxy_config in proxies_to_try:\n",
    "#         browser = None\n",
    "#         try:\n",
    "#             isbri_data = False\n",
    "#             if proxy_config == PROXY_URL:\n",
    "#                 logging.info(\"Attempting with bri-data proxy (allowed by robots.txt)\")\n",
    "#                 print(\"Attempting with bri-data proxy (allowed by robots.txt)\")\n",
    "#                 browser = await p.chromium.connect_over_cdp(PROXY_URL)\n",
    "#                 isbri_data = True\n",
    "#             else:\n",
    "#                 logging.info(\"Attempting with oxylabs proxy (required by robots.txt)\")\n",
    "#                 browser = await p.chromium.launch(\n",
    "#                     proxy=proxy_config,\n",
    "#                     headless=True,\n",
    "#                     args=[\n",
    "#                         '--disable-blink-features=AutomationControlled',\n",
    "#                         '--disable-web-security'\n",
    "#                     ]\n",
    "#                 )\n",
    "\n",
    "#             context = await browser.new_context()\n",
    "#             await context.add_init_script(\"\"\"\n",
    "#                 Object.defineProperty(navigator, 'webdriver', {\n",
    "#                     get: () => undefined\n",
    "#                 })\n",
    "#             \"\"\")\n",
    "#             page = await context.new_page()\n",
    "            \n",
    "#             await safe_goto_and_wait(page, url,isbri_data)\n",
    "#             return browser, page\n",
    "\n",
    "#         except Exception as e:\n",
    "#             last_error = e\n",
    "#             error_trace = traceback.format_exc()\n",
    "#             logging.error(f\"Proxy attempt failed:\\n{error_trace}\")\n",
    "#             print(f\"Proxy attempt failed:\\n{error_trace}\")\n",
    "#             if browser:\n",
    "#                 await browser.close()\n",
    "#             continue\n",
    "\n",
    "#     error_msg = (f\"Failed to load {url} using all proxy options. \"\n",
    "#                 f\"Last error: {str(last_error)}\\n\"\n",
    "#                 f\"URL may be disallowed by robots.txt or proxies failed.\")\n",
    "#     logging.error(error_msg)\n",
    "#     print(error_msg)\n",
    "#     raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# async def get_robots_txt_rules(base_url: str) -> List[str]:\n",
    "#     \"\"\"Dynamically fetch and parse robots.txt rules\"\"\"\n",
    "#     robots_url = f\"{base_url}/robots.txt\"\n",
    "#     try:\n",
    "#         async with httpx.AsyncClient() as client:\n",
    "#             resp = await client.get(robots_url, timeout=10)\n",
    "#             if resp.status_code == 200:\n",
    "#                 return [\n",
    "#                     line.split(\":\", 1)[1].strip()\n",
    "#                     for line in resp.text.splitlines()\n",
    "#                     if line.lower().startswith(\"disallow:\")\n",
    "#                 ]\n",
    "#     except Exception as e:\n",
    "#         logging.warning(f\"Couldn't fetch robots.txt: {e}\")\n",
    "#         print(\"Couldnt fetch robots.txt:\", {e})\n",
    "#     return []\n",
    "\n",
    "\n",
    "# def check_url_against_rules(url: str, disallowed_patterns: List[str]) -> bool:\n",
    "#     \"\"\"Check if URL matches any robots.txt disallowed pattern\"\"\"\n",
    "#     for pattern in disallowed_patterns:\n",
    "#         try:\n",
    "#             # Handle wildcard patterns\n",
    "#             if \"*\" in pattern:\n",
    "#                 regex_pattern = pattern.replace(\"*\", \".*\")\n",
    "#                 if re.search(regex_pattern, url):\n",
    "#                     return True\n",
    "#             # Handle path patterns\n",
    "#             elif url.startswith(f\"{pattern}\"):\n",
    "#                 return True\n",
    "#             # Handle query parameters\n",
    "#             elif (\"?\" in url) and any(\n",
    "#                 f\"{param}=\" in url \n",
    "#                 for param in pattern.split(\"=\")[0].split(\"*\")[-1:]\n",
    "#                 if \"=\" in pattern\n",
    "#             ):\n",
    "#                 return True\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"Error checking pattern {pattern}: {e}\")\n",
    "            \n",
    "#     return False\n",
    "\n",
    "\n",
    "\n",
    "# def build_url_with_loadmore(base_url: str, page_count: int) -> str:\n",
    "#     separator = '&' if '?' in base_url else '?'\n",
    "#     return f\"{base_url}{separator}loadMore={page_count}\"   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# async def scrape_product_detail(url):\n",
    "#     title = sku = final_output_price = protection_plan = monthly_payment = result = review_summary = \"N/A\"\n",
    "#     ring_sizes = []\n",
    "#     image_urls = []\n",
    "\n",
    "#     try:\n",
    "#         async with async_playwright() as p:\n",
    "#             browser = await p.chromium.launch(headless=False)\n",
    "#             context = await browser.new_context()\n",
    "#             # context = await browser.new_context(\n",
    "#             #     proxy={\n",
    "#             #         \"server\": PROXY_SERVER,\n",
    "#             #         \"username\": PROXY_USERNAME,\n",
    "#             #         \"password\": PROXY_PASSWORD\n",
    "#             #     }\n",
    "#             # )\n",
    "\n",
    "#             page = await context.new_page()\n",
    "#             logging.info(\"Opening page...\")\n",
    "#             await page.goto(url, timeout=60000)\n",
    "#             logging.info(f\"Successfully accessed {url}\")\n",
    "\n",
    "#             # Wait for critical elements to load\n",
    "#             try:\n",
    "#                 logging.info(\"Scrolling from top to bottom to load all lazy-loaded content...\")\n",
    "#                 previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            \n",
    "#                 while True:\n",
    "#                     await page.evaluate(\"window.scrollBy(0, 1000)\")  # scroll in steps\n",
    "#                     await asyncio.sleep(0.5)  # allow lazy elements to load\n",
    "            \n",
    "#                     new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "#                     if new_height == previous_height:\n",
    "#                         break  # reached the bottom\n",
    "#                     previous_height = new_height\n",
    "            \n",
    "#                 logging.info(\"Scrolling completed.\")\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"Scrolling failed: {e}\")\n",
    "\n",
    "#             # Wait and click the \"No, thanks\" button using aria-label\n",
    "#             try:\n",
    "#                 await page.wait_for_selector('button[aria-label=\"No, thanks; close the dialog\"]', timeout=5000)\n",
    "#                 await page.click('button[aria-label=\"No, thanks; close the dialog\"]')\n",
    "#                 print(\"Clicked the 'No, thanks' button.\")\n",
    "#             except:\n",
    "#                 print(\"Button not found or already closed.\")\n",
    "\n",
    "\n",
    "#             # SKU Extraction\n",
    "#             try:\n",
    "#                 sku_el = await page.query_selector(\".product-detail__intro--productcode\")\n",
    "#                 if sku_el:\n",
    "#                     text = (await sku_el.inner_text()).strip()\n",
    "#                     if \"Item #:\" in text:\n",
    "#                         sku = text.split(\"Item #:\")[1].strip()\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"[SKU Extraction Error] {e}\")\n",
    "\n",
    "#             # Images\n",
    "#             base_url = \"https://www.kay.com\"\n",
    "#             try:\n",
    "#                 img_elements = await page.query_selector_all(\".swiper-slide img\")\n",
    "#                 for img in img_elements:\n",
    "#                     src = await img.get_attribute(\"src\")\n",
    "#                     if src and src.startswith(\"/productimages/processed\"):\n",
    "#                         image_urls.append(base_url + src)\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"[Image Extraction Error] {e}\")\n",
    "\n",
    "#             # Title\n",
    "#             try:\n",
    "#                 title = await page.locator(\"div.product-detail__summary--name h1\").inner_text()\n",
    "#             except:\n",
    "#                 title = \"N/A\"\n",
    "\n",
    "#             # Prices and Discount\n",
    "#             try:\n",
    "#                 discounted_price = await page.locator(\"span.product-price__price\").inner_text()\n",
    "#             except:\n",
    "#                 discounted_price = \"N/A\"\n",
    "            \n",
    "#             try:\n",
    "#                 original_price = await page.locator(\"span.product-price__striked\").inner_text()\n",
    "#             except:\n",
    "#                 original_price = None\n",
    "            \n",
    "#             try:\n",
    "#                 discount = await page.locator(\"span.tag-text\").inner_text()\n",
    "#             except:\n",
    "#                 discount = None\n",
    "            \n",
    "#             # Construct final string smartly\n",
    "#             price_info = f\"Discounted Price: {discounted_price.strip()}\"\n",
    "            \n",
    "#             price_info += f\" | Original Price: {original_price.strip() if original_price else 'N/A'}\"\n",
    "#             price_info += f\" | Discount: {discount.strip() if discount else 'N/A'}\"\n",
    "            \n",
    "#             final_output_price = price_info\n",
    "\n",
    "#             # Monthly Payment Full Sentence Extraction (Cleaned)\n",
    "#             try:\n",
    "#                 # Ensure the element is present\n",
    "#                 await page.wait_for_selector(\"div.ep-text-placement\", timeout=10000)\n",
    "                \n",
    "#                 # Locate and extract the full sentence\n",
    "#                 element = page.locator(\"div.ep-text-placement\")\n",
    "#                 monthly_payment = await element.text_content()  # use text_content for reliability\n",
    "                \n",
    "#                 if monthly_payment:\n",
    "#                     monthly_payment = monthly_payment.replace(\"\\n\", \" \").strip()\n",
    "#                     monthly_payment = re.sub(r'[\\s\\.]*\\^[\\s\\.]*', '', monthly_payment)  # remove footnote marker\n",
    "#                     monthly_payment = re.sub(r'\\s{2,}', ' ', monthly_payment)           # normalize spaces\n",
    "#                 else:\n",
    "#                     monthly_payment = \"N/A\"\n",
    "            \n",
    "#                 print(\"Monthly Payment:\", monthly_payment)\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error extracting monthly payment: {str(e)}\")\n",
    "#                 monthly_payment = \"N/A\"\n",
    "\n",
    "\n",
    "#             # Ring Sizes\n",
    "#             try:\n",
    "#                 # Wait until ring size selector is loaded\n",
    "#                 await page.wait_for_selector(\"div.ring-size-selector label.selector-label\", timeout=10000)\n",
    "                \n",
    "#                 # Get all visible ring size texts\n",
    "#                 size_elements = await page.locator(\"div.ring-size-selector label.selector-label\").all_inner_texts()\n",
    "            \n",
    "#                 # Clean each size (remove asterisks and extra whitespace)\n",
    "#                 ring_sizes = [s.strip().replace(\"*\", \"\") for s in size_elements]\n",
    "            \n",
    "#                 # print(\"Extracted Ring Sizes:\", ring_sizes)\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"Failed to extract ring sizes: {e}\")\n",
    "#                 ring_sizes = []\n",
    "\n",
    "\n",
    "            \n",
    "#             # # Protection Plan\n",
    "#             # try:\n",
    "#             #     # Extract header title and subtitle\n",
    "#             #     title_block = await page.locator(\"div.warranty_heading span\").all_inner_texts()\n",
    "#             #     main_title = title_block[0].strip() if len(title_block) > 0 else \"\"\n",
    "#             #     subtitle = title_block[1].strip() if len(title_block) > 1 else \"\"\n",
    "#             #     header = f\"{main_title} \\\\n{subtitle} \\\\n\\\\n\"\n",
    "            \n",
    "#             #     # Extract service rows\n",
    "#             #     rows = await page.locator(\"table.table tbody tr\").all()\n",
    "#             #     services = []\n",
    "#             #     for row in rows:\n",
    "#             #         cols = await row.locator(\"td\").all_inner_texts()\n",
    "#             #         if len(cols) >= 3:\n",
    "#             #             service = (\n",
    "#             #                 f\"Service Needed: {cols[0].strip()} \\\\n\"\n",
    "#             #                 f\"Typical Cost: {cols[1].strip()} \\\\n\"\n",
    "#             #                 f\"With This Plan: {cols[2].strip()} \\\\n\\\\n\"\n",
    "#             #             )\n",
    "#             #             services.append(service)\n",
    "            \n",
    "#             #     # Join everything into one string\n",
    "#             #     protection_plan = header + \"\".join(services).strip()\n",
    "            \n",
    "#             # except Exception as e:\n",
    "#             #     protection_plan = f\"Error extracting protection plan: {e}\"\n",
    "\n",
    "#             try:\n",
    "#                 # Extract header title and subtitle\n",
    "#                 title_block = await page.locator(\"div.warranty_heading span\").all_inner_texts()\n",
    "#                 main_title = title_block[0].strip() if len(title_block) > 0 else \"\"\n",
    "#                 subtitle = title_block[1].strip() if len(title_block) > 1 else \"\"\n",
    "            \n",
    "#                 protection_plan_data = {\n",
    "#                     \"title\": main_title,\n",
    "#                     \"subtitle\": subtitle,\n",
    "#                     \"services\": []\n",
    "#                 }\n",
    "            \n",
    "#                 # Extract service rows\n",
    "#                 rows = await page.locator(\"table.table tbody tr\").all()\n",
    "#                 for row in rows:\n",
    "#                     cols = await row.locator(\"td\").all_inner_texts()\n",
    "#                     if len(cols) >= 3:\n",
    "#                         protection_plan_data[\"services\"].append({\n",
    "#                             \"service_needed\": cols[0].strip(),\n",
    "#                             \"typical_cost\": cols[1].strip(),\n",
    "#                             \"with_plan_cost\": cols[2].strip()\n",
    "#                         })\n",
    "            \n",
    "#                 # Convert to JSON string for storage\n",
    "#                 protection_plan = json.dumps(protection_plan_data, ensure_ascii=False)\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 protection_plan = json.dumps({\"error\": f\"Error extracting protection plan: {str(e)}\"})\n",
    "\n",
    "          \n",
    "            \n",
    "#             # # Product Specifications\n",
    "#             # try:\n",
    "#             #     result = \"\"\n",
    "#             #     seen_sections = set()\n",
    "            \n",
    "#             #     # 1. Expand 'Details' accordion if collapsed\n",
    "#             #     try:\n",
    "#             #         details_button = await page.query_selector('button#Details[aria-expanded=\"false\"]')\n",
    "#             #         if details_button:\n",
    "#             #             await details_button.click()\n",
    "#             #             # Wait for accordion content to appear rather than specific tables\n",
    "#             #             await page.wait_for_selector(\"div.accordion-body\", state=\"visible\", timeout=8000)\n",
    "#             #             await asyncio.sleep(1)\n",
    "#             #             logging.info(\"Expanded 'Details' accordion\")\n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Could not expand Details accordion: {e}\")\n",
    "            \n",
    "#             #     # 2. More resilient table handling\n",
    "#             #     tables = []\n",
    "#             #     try:\n",
    "#             #         # Try direct access without waiting\n",
    "#             #         tables = await page.query_selector_all(\"table.specs-table\")\n",
    "                    \n",
    "#             #         # If no tables found, try waiting briefly\n",
    "#             #         if not tables:\n",
    "#             #             try:\n",
    "#             #                 await page.wait_for_selector(\"table.specs-table\", state=\"attached\", timeout=3000)\n",
    "#             #                 tables = await page.query_selector_all(\"table.specs-table\")\n",
    "#             #             except Exception:\n",
    "#             #                 pass\n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Table query error: {e}\")\n",
    "            \n",
    "#             #     if not tables:\n",
    "#             #         result = \"No specifications found\"\n",
    "#             #         logging.info(\"No specification tables found\")\n",
    "#             #     else:\n",
    "#             #         logging.info(f\"Found {len(tables)} specification tables\")\n",
    "                    \n",
    "#             #         for table in tables:\n",
    "#             #             try:\n",
    "#             #                 # Section header (Stone, Metal, etc.)\n",
    "#             #                 section_title = \"Specifications\"\n",
    "                            \n",
    "#             #                 # Try different header locations\n",
    "#             #                 section_title_elem = await table.query_selector(\"thead th[colspan]\")\n",
    "#             #                 if not section_title_elem:\n",
    "#             #                     section_title_elem = await table.query_selector(\"caption\")\n",
    "#             #                 if not section_title_elem:\n",
    "#             #                     section_title_elem = await table.query_selector(\"thead tr:first-child th\")\n",
    "                            \n",
    "#             #                 if section_title_elem:\n",
    "#             #                     section_title = (await section_title_elem.inner_text()).strip()\n",
    "                            \n",
    "#             #                 # Skip duplicate sections\n",
    "#             #                 if section_title in seen_sections:\n",
    "#             #                     continue\n",
    "#             #                 seen_sections.add(section_title)\n",
    "            \n",
    "#             #                 result += f\"{section_title}:\\n\"\n",
    "            \n",
    "#             #                 # Process key-value pairs\n",
    "#             #                 rows = await table.query_selector_all(\"tbody tr\")\n",
    "#             #                 for row in rows:\n",
    "#             #                     try:\n",
    "#             #                         cells = await row.query_selector_all(\"td\")\n",
    "#             #                         if len(cells) >= 2:\n",
    "#             #                             key = (await cells[0].inner_text()).strip()\n",
    "#             #                             value = (await cells[1].inner_text()).strip()\n",
    "                                        \n",
    "#             #                             # Skip empty rows\n",
    "#             #                             if key or value:\n",
    "#             #                                 result += f\"{key}: {value}\\n\"\n",
    "#             #                     except Exception:\n",
    "#             #                         continue  # Skip problematic rows\n",
    "            \n",
    "#             #                 result += \"\\n\"\n",
    "                            \n",
    "#             #             except Exception as e:\n",
    "#             #                 logging.warning(f\"Error processing table: {e}\")\n",
    "#             #                 continue\n",
    "            \n",
    "#             #     if not result.strip():\n",
    "#             #         result = \"No specifications found\"\n",
    "            \n",
    "#             # except Exception as e:\n",
    "#             #     result = f\"Specifications error: {str(e)}\"\n",
    "#             #     logging.error(f\"Specifications extraction failed: {e}\")\n",
    "\n",
    "#             # Product Specifications\n",
    "#             try:\n",
    "#                 product_specifications = {}\n",
    "#                 seen_sections = set()\n",
    "            \n",
    "#                 # 1. Expand 'Details' accordion if collapsed\n",
    "#                 try:\n",
    "#                     details_button = await page.query_selector('button#Details[aria-expanded=\"false\"]')\n",
    "#                     if details_button:\n",
    "#                         await details_button.click()\n",
    "#                         await page.wait_for_selector(\"div.accordion-body\", state=\"visible\", timeout=8000)\n",
    "#                         await asyncio.sleep(1)\n",
    "#                         logging.info(\"Expanded 'Details' accordion\")\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Could not expand Details accordion: {e}\")\n",
    "            \n",
    "#                 # 2. More resilient table handling\n",
    "#                 tables = []\n",
    "#                 try:\n",
    "#                     tables = await page.query_selector_all(\"table.specs-table\")\n",
    "#                     if not tables:\n",
    "#                         await page.wait_for_selector(\"table.specs-table\", state=\"attached\", timeout=3000)\n",
    "#                         tables = await page.query_selector_all(\"table.specs-table\")\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Table query error: {e}\")\n",
    "            \n",
    "#                 if not tables:\n",
    "#                     product_specifications = {\"error\": \"No specifications found\"}\n",
    "#                     logging.info(\"No specification tables found\")\n",
    "#                 else:\n",
    "#                     logging.info(f\"Found {len(tables)} specification tables\")\n",
    "            \n",
    "#                     for table in tables:\n",
    "#                         try:\n",
    "#                             # Default section title\n",
    "#                             section_title = \"Specifications\"\n",
    "            \n",
    "#                             # Try different header locations\n",
    "#                             section_title_elem = await table.query_selector(\"thead th[colspan]\") or \\\n",
    "#                                                   await table.query_selector(\"caption\") or \\\n",
    "#                                                   await table.query_selector(\"thead tr:first-child th\")\n",
    "            \n",
    "#                             if section_title_elem:\n",
    "#                                 section_title = (await section_title_elem.inner_text()).strip()\n",
    "            \n",
    "#                             if section_title in seen_sections:\n",
    "#                                 continue\n",
    "#                             seen_sections.add(section_title)\n",
    "            \n",
    "#                             section_data = {}\n",
    "            \n",
    "#                             rows = await table.query_selector_all(\"tbody tr\")\n",
    "#                             for row in rows:\n",
    "#                                 try:\n",
    "#                                     cells = await row.query_selector_all(\"td\")\n",
    "#                                     if len(cells) >= 2:\n",
    "#                                         key = (await cells[0].inner_text()).strip()\n",
    "#                                         value = (await cells[1].inner_text()).strip()\n",
    "#                                         if key or value:\n",
    "#                                             section_data[key] = value\n",
    "#                                 except Exception:\n",
    "#                                     continue  # Skip problematic rows\n",
    "            \n",
    "#                             if section_data:\n",
    "#                                 product_specifications[section_title] = section_data\n",
    "            \n",
    "#                         except Exception as e:\n",
    "#                             logging.warning(f\"Error processing table: {e}\")\n",
    "#                             continue\n",
    "            \n",
    "#                 if not product_specifications:\n",
    "#                     product_specifications = {\"message\": \"No specifications found\"}\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 product_specifications = {\"error\": f\"Specifications extraction failed: {str(e)}\"}\n",
    "#                 logging.error(f\"Specifications extraction failed: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "#             # Reviews Extraction with \\n in output\n",
    "#             # try:\n",
    "#             #     review_summary = \"\"\n",
    "                \n",
    "#             #     # Expand REVIEWS accordion if collapsed\n",
    "#             #     try:\n",
    "#             #         reviews_button = await page.query_selector('button#Reviews[aria-expanded=\"false\"]')\n",
    "#             #         if reviews_button:\n",
    "#             #             await reviews_button.click()\n",
    "#             #             # Wait specifically for reviews content to appear\n",
    "#             #             await page.wait_for_selector(\"app-pdp-reviews-display\", state=\"visible\", timeout=8000)\n",
    "#             #             await asyncio.sleep(1)\n",
    "#             #             logging.info(\"Expanded 'Reviews' accordion successfully.\")\n",
    "#             #         else:\n",
    "#             #             logging.info(\"Reviews accordion already expanded or not found\")\n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Failed to expand 'Reviews' accordion: {e}\")\n",
    "                \n",
    "#             #     # Extract review summary\n",
    "#             #     try:\n",
    "#             #         review_text = await page.locator(\".pdp-review-stars__desc\").inner_text()\n",
    "#             #         overall_rating = await page.locator(\".pdp-review-stars-rating__count\").inner_text()\n",
    "                    \n",
    "#             #         review_summary += f\"{review_text.strip()}\\n\"\n",
    "#             #         review_summary += f\"Overall Rating: {overall_rating.strip()}\\n\\n\"\n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Review summary extraction failed: {e}\")\n",
    "#             #         review_summary += \"Review summary not available\\n\\n\"\n",
    "                \n",
    "#             #     # Ratings Overview\n",
    "#             #     try:\n",
    "#             #         review_summary += \"Ratings Overview:\\n\"\n",
    "#             #         star_rows = await page.locator(\".pdp-review-breakdown__second-ratings\").all()\n",
    "#             #         for row in star_rows:\n",
    "#             #             stars = await row.locator(\".second-ratings-name\").inner_text()\n",
    "#             #             count = await row.locator(\".second-rating-count\").inner_text()\n",
    "#             #             review_summary += f\"{stars.strip()}: {count.strip()}\\n\"\n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Ratings overview extraction failed: {e}\")\n",
    "#             #         review_summary += \"Ratings overview not available\\n\"\n",
    "                \n",
    "#             #     # Ratings Breakdown\n",
    "#             #     try:\n",
    "#             #         review_summary += \"\\nRatings Breakdown:\\n\"\n",
    "#             #         # Use the correct container for breakdown ratings\n",
    "#             #         breakdown_rows = await page.locator(\"app-signet-pdp-rating-breakdown .pdp-review-breakdown__second-ratings\").all()\n",
    "#             #         for row in breakdown_rows[:3]:  # Only first 3 categories\n",
    "#             #             category = await row.locator(\".second-ratings-name\").inner_text()\n",
    "#             #             rating = await row.locator(\".second-rating-count\").inner_text()\n",
    "#             #             review_summary += f\"{category.strip()}: {rating.strip()}\\n\"\n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Ratings breakdown extraction failed: {e}\")\n",
    "#             #         review_summary += \"Ratings breakdown not available\\n\"\n",
    "                \n",
    "#                 # Customer Reviews\n",
    "#             #     try:\n",
    "#             #         review_summary += \"\\nCustomer Reviews:\\n\"\n",
    "#             #         reviews = await page.locator(\".pdp-review-display__review\").all()\n",
    "                    \n",
    "#             #         if not reviews:\n",
    "#             #             review_summary += \"No reviews found\\n\"\n",
    "#             #         else:\n",
    "#             #             for i, review in enumerate(reviews[:5]):  # Limit to first 5 reviews\n",
    "#             #                 try:\n",
    "#             #                     # Basic info\n",
    "#             #                     reviewer = await review.locator(\".pdp-review-display__name\").inner_text()\n",
    "#             #                     date = await review.locator(\".pdp-review-display__time\").inner_text()\n",
    "#             #                     stars = len(await review.locator(\".fa-Star-Rated\").all())\n",
    "                                \n",
    "#             #                     # Badges\n",
    "#             #                     badges = []\n",
    "#             #                     badge_elements = await review.locator(\".pdp-review-display__review-badge-text\").all()\n",
    "#             #                     for badge in badge_elements:\n",
    "#             #                         badge_text = await badge.inner_text()\n",
    "#             #                         if badge_text.strip():\n",
    "#             #                             badges.append(badge_text.strip())\n",
    "                                \n",
    "#             #                     # Content\n",
    "#             #                     title = await review.locator(\".pdp-review-display__title\").inner_text() or \"No Title\"\n",
    "#             #                     content = await review.locator(\".pdp-review-display__content\").inner_text()\n",
    "                                \n",
    "#             #                     # Recommendation\n",
    "#             #                     recommend = \"Yes\" if await review.locator(\".pdp-review-display__recommend\").count() else \"No\"\n",
    "                                \n",
    "#             #                     # Helpful counts\n",
    "#             #                     helpful_text = await review.locator(\".pdp-review-display__helpful\").inner_text()\n",
    "#             #                     yes_match = re.search(r'Yes \\((\\d+)\\)', helpful_text)\n",
    "#             #                     no_match = re.search(r'No \\((\\d+)\\)', helpful_text)\n",
    "#             #                     yes_count = yes_match.group(1) if yes_match else \"0\"\n",
    "#             #                     no_count = no_match.group(1) if no_match else \"0\"\n",
    "                                \n",
    "#             #                     # Image count\n",
    "#             #                     images = await review.locator(\".pdp-review-display__images img\").count()\n",
    "                                \n",
    "#             #                     # Format review\n",
    "#             #                     review_summary += (\n",
    "#             #                         f\"\\nReview {i+1}:\\n\"\n",
    "#             #                         f\"Reviewer: {reviewer.strip()}\\n\"\n",
    "#             #                         f\"Date: {date.strip()}\\n\"\n",
    "#             #                         f\"Rating: {'' * stars}\\n\"\n",
    "#             #                         f\"Badges: {', '.join(badges) if badges else 'None'}\\n\"\n",
    "#             #                         f\"Title: {title.strip()}\\n\"\n",
    "#             #                         f\"Content: {content.strip()}\\n\"\n",
    "#             #                         f\"Recommends: {recommend}\\n\"\n",
    "#             #                         f\"Helpful: Yes({yes_count}), No({no_count})\\n\"\n",
    "#             #                         f\"Images: {images}\\n\"\n",
    "#             #                         f\"{'-'*40}\\n\"\n",
    "#             #                     )\n",
    "#             #                 except Exception as e:\n",
    "#             #                     logging.warning(f\"Error processing review {i+1}: {e}\")\n",
    "#             #                     review_summary += f\"\\nReview {i+1}: Extraction failed\\n{'-'*40}\\n\"\n",
    "                \n",
    "#             #     except Exception as e:\n",
    "#             #         logging.warning(f\"Customer reviews extraction failed: {e}\")\n",
    "#             #         review_summary += \"Customer reviews not available\\n\"\n",
    "            \n",
    "#             # except Exception as review_error:\n",
    "#             #     logging.error(f\"[Review Extraction Error] {review_error}\")\n",
    "#             #     review_summary = \"Review information not available.\"\n",
    "\n",
    "            \n",
    "            \n",
    "#             try:\n",
    "#                 # Expand REVIEWS accordion if collapsed\n",
    "#                 review_data = {\n",
    "#                     \"summary\": {},\n",
    "#                     \"overview\": {},\n",
    "#                     \"breakdown\": {},\n",
    "#                     \"reviews\": []\n",
    "#                 }\n",
    "#                 try:\n",
    "#                     reviews_button = await page.query_selector('button#Reviews[aria-expanded=\"false\"]')\n",
    "#                     if reviews_button:\n",
    "#                         await reviews_button.click()\n",
    "#                         await page.wait_for_selector(\"app-pdp-reviews-display\", state=\"visible\", timeout=8000)\n",
    "#                         await asyncio.sleep(1)\n",
    "#                         logging.info(\"Expanded 'Reviews' accordion successfully.\")\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Failed to expand 'Reviews' accordion: {e}\")\n",
    "            \n",
    "#                 # Extract review summary text and overall rating\n",
    "#                 try:\n",
    "#                     review_text = await page.locator(\".pdp-review-stars__desc\").inner_text()\n",
    "#                     overall_rating = await page.locator(\".pdp-review-stars-rating__count\").inner_text()\n",
    "#                     review_data[\"summary\"] = {\n",
    "#                         \"text\": review_text.strip(),\n",
    "#                         \"overall_rating\": overall_rating.strip()\n",
    "#                     }\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Review summary extraction failed: {e}\")\n",
    "#                     review_data[\"summary\"] = {\n",
    "#                         \"text\": \"Review summary not available\",\n",
    "#                         \"overall_rating\": \"N/A\"\n",
    "#                     }\n",
    "            \n",
    "#                 # Ratings Overview\n",
    "#                 try:\n",
    "#                     star_rows = await page.locator(\".pdp-review-breakdown__second-ratings\").all()\n",
    "#                     for row in star_rows:\n",
    "#                         stars = await row.locator(\".second-ratings-name\").inner_text()\n",
    "#                         count = await row.locator(\".second-rating-count\").inner_text()\n",
    "#                         review_data[\"overview\"][stars.strip()] = count.strip()\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Ratings overview extraction failed: {e}\")\n",
    "            \n",
    "#                 # Ratings Breakdown\n",
    "#                 try:\n",
    "#                     breakdown_rows = await page.locator(\"app-signet-pdp-rating-breakdown .pdp-review-breakdown__second-ratings\").all()\n",
    "#                     for row in breakdown_rows[:3]:\n",
    "#                         category = await row.locator(\".second-ratings-name\").inner_text()\n",
    "#                         rating = await row.locator(\".second-rating-count\").inner_text()\n",
    "#                         review_data[\"breakdown\"][category.strip()] = rating.strip()\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Ratings breakdown extraction failed: {e}\")\n",
    "            \n",
    "#                 # Customer Reviews\n",
    "#                 try:\n",
    "#                     reviews = await page.locator(\".pdp-review-display__review\").all()\n",
    "#                     for review in reviews[:5]:  # limit to 5\n",
    "#                         try:\n",
    "#                             reviewer = await review.locator(\".pdp-review-display__name\").inner_text()\n",
    "#                             date = await review.locator(\".pdp-review-display__time\").inner_text()\n",
    "#                             stars = len(await review.locator(\".fa-Star-Rated\").all())\n",
    "                            \n",
    "#                             # Badges\n",
    "#                             badges = []\n",
    "#                             badge_elements = await review.locator(\".pdp-review-display__review-badge-text\").all()\n",
    "#                             for badge in badge_elements:\n",
    "#                                 badge_text = await badge.inner_text()\n",
    "#                                 if badge_text.strip():\n",
    "#                                     badges.append(badge_text.strip())\n",
    "            \n",
    "#                             # Content\n",
    "#                             title = await review.locator(\".pdp-review-display__title\").inner_text() or \"No Title\"\n",
    "#                             content = await review.locator(\".pdp-review-display__content\").inner_text()\n",
    "            \n",
    "#                             # Recommendation\n",
    "#                             recommend = bool(await review.locator(\".pdp-review-display__recommend\").count())\n",
    "            \n",
    "#                             # Helpful counts\n",
    "#                             helpful_text = await review.locator(\".pdp-review-display__helpful\").inner_text()\n",
    "#                             yes_match = re.search(r'Yes \\((\\d+)\\)', helpful_text)\n",
    "#                             no_match = re.search(r'No \\((\\d+)\\)', helpful_text)\n",
    "#                             yes_count = int(yes_match.group(1)) if yes_match else 0\n",
    "#                             no_count = int(no_match.group(1)) if no_match else 0\n",
    "            \n",
    "#                             # Image count\n",
    "#                             images = await review.locator(\".pdp-review-display__images img\").count()\n",
    "            \n",
    "#                             review_data[\"reviews\"].append({\n",
    "#                                 \"reviewer\": reviewer.strip(),\n",
    "#                                 \"date\": date.strip(),\n",
    "#                                 \"rating\": stars,\n",
    "#                                 \"badges\": badges,\n",
    "#                                 \"title\": title.strip(),\n",
    "#                                 \"content\": content.strip(),\n",
    "#                                 \"recommends\": recommend,\n",
    "#                                 \"helpful\": {\"yes\": yes_count, \"no\": no_count},\n",
    "#                                 \"images\": images\n",
    "#                             })\n",
    "            \n",
    "#                         except Exception as e:\n",
    "#                             logging.warning(f\"Error processing individual review: {e}\")\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Customer reviews extraction failed: {e}\")\n",
    "            \n",
    "#             except Exception as review_error:\n",
    "#                 logging.error(f\"[Review Extraction Error] {review_error}\")\n",
    "#                 review_data = {\n",
    "#                     \"summary\": {\"text\": \"Review information not available\", \"overall_rating\": \"N/A\"},\n",
    "#                     \"overview\": {},\n",
    "#                     \"breakdown\": {},\n",
    "#                     \"reviews\": []\n",
    "#                 }\n",
    "            \n",
    "#             # Convert to JSON string for DB\n",
    "#             review_summary = json.dumps(review_data, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             await browser.close()\n",
    "\n",
    "#             return {\n",
    "#                 \"title\": title,\n",
    "#                 \"sku\": sku,\n",
    "#                 \"final_output_price\": final_output_price,\n",
    "#                 \"ring_sizes\": ring_sizes,\n",
    "#                 \"protection_plan\": protection_plan,\n",
    "#                 \"monthly_payment\": monthly_payment,\n",
    "#                 \"product_details\": product_specifications,\n",
    "#                 \"review_summary\": review_summary,\n",
    "#                 \"image_urls\": image_urls\n",
    "#             }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"[Detail Error] {url}: {e}\")\n",
    "#         return {\n",
    "#             \"title\": title,\n",
    "#             \"sku\": sku,\n",
    "#             \"final_output_price\": final_output_price,\n",
    "#             \"ring_sizes\": ring_sizes,\n",
    "#             \"protection_plan\": protection_plan,\n",
    "#             \"monthly_payment\": monthly_payment,\n",
    "#             \"product_details\": product_specifications,\n",
    "#             \"review_summary\": review_summary,\n",
    "#             \"image_urls\": image_urls\n",
    "#         }\n",
    "\n",
    "        \n",
    "# ########################################  Main Function Call ####################################################################\n",
    "# async def handle_kay(url, max_pages):\n",
    "#     print(\"============================\")\n",
    "#     print(url)\n",
    "#     print(\"============================\")\n",
    "#     # Prepare directories and files\n",
    "#     os.makedirs(EXCEL_DATA_PATH, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     image_folder = os.path.join(IMAGE_SAVE_PATH, timestamp)\n",
    "#     os.makedirs(image_folder, exist_ok=True)\n",
    "#     # Create workbook and setup\n",
    "#     wb = Workbook()\n",
    "#     sheet = wb.active\n",
    "#     sheet.title = \"Products\"\n",
    "#     headers = [\"Current Date\", \"Header\", \"Product Name\", \"Image\", \"Kt\", \"Price\", \"Total Dia wt\", \"Time\", \"ImagePath\", \"Additional Info\"]\n",
    "#     sheet.append(headers)\n",
    "\n",
    "#     all_records = []\n",
    "#     filename = f\"Kay_{datetime.now().strftime('%Y-%m-%d_%H.%M')}.xlsx\"\n",
    "#     file_path = os.path.join(EXCEL_DATA_PATH, filename)\n",
    "\n",
    "#     page_count = 0\n",
    "#     success_count = 0\n",
    "\n",
    "#     async with async_playwright() as p:\n",
    "#         while page_count < max_pages:\n",
    "#             current_url = build_url_with_loadmore(url, page_count)\n",
    "#             # logging.info(f\"Processing page {page_count}: {current_url}\")\n",
    "#             browser = None\n",
    "#             page = None\n",
    "            \n",
    "#             try:\n",
    "#                 # Use the new proxy strategy function\n",
    "#                 browser, page = await get_browser_with_proxy_strategy(p, current_url)\n",
    "#                 log_event(f\"Successfully loaded: {current_url}\")\n",
    "#                 # Scroll to load all products\n",
    "#                 prev_product_count = 0\n",
    "#                 for _ in range(10):\n",
    "#                     await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "#                     await asyncio.sleep(random.uniform(1, 2))  # Random delay between scrolls\n",
    "#                     current_product_count = await page.locator('.product-item').count()\n",
    "#                     if current_product_count == prev_product_count:\n",
    "#                         break\n",
    "#                     prev_product_count = current_product_count\n",
    "\n",
    "\n",
    "#                 product_wrapper = await page.query_selector(\"div.product-scroll-wrapper\")\n",
    "#                 products = await product_wrapper.query_selector_all(\"div.product-item\") if product_wrapper else []\n",
    "#                 logging.info(f\"Total products found on page {page_count}: {len(products)}\")\n",
    "\n",
    "#                 page_title = await page.title()\n",
    "#                 current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#                 time_only = datetime.now().strftime(\"%H.%M\")\n",
    "\n",
    "#                 records = []\n",
    "#                 image_tasks = []\n",
    "#                 print(\"No of products in Portal\",len(products))\n",
    "\n",
    "#                 for row_num, product in enumerate(products, start=len(sheet[\"A\"]) + 1):\n",
    "\n",
    "#                     print(\"Record Number :\",row_num-1)\n",
    "\n",
    "#                     try:\n",
    "#                         base_url = \"https://www.kay.com\"\n",
    "#                         product_link_element = await product.query_selector(\"a.thumb.main-thumb\")\n",
    "#                         product_href = await product_link_element.get_attribute(\"href\")\n",
    "#                         if product_href:\n",
    "#                             product_url = base_url + product_href\n",
    "                           \n",
    "#                         else:\n",
    "#                             product_url = \"N/A\"\n",
    "#                     except:\n",
    "#                         product_url = \"N/A\"\n",
    "                    \n",
    "#                     try:\n",
    "#                         product_name = await (await product.query_selector(\"h2.name.product-tile-description\")).inner_text()\n",
    "#                     except:\n",
    "#                         product_name = \"N/A\"\n",
    "\n",
    "#                     try:\n",
    "#                         # Extract current price (the offer price if available)\n",
    "#                         price_el = await product.query_selector(\"div.price\")\n",
    "#                         current_price_text = await price_el.inner_text() if price_el else \"\"\n",
    "#                         #print(f\"Current Price Text: {current_price_text}\")  # Debugging\n",
    "#                         current_price = current_price_text.strip().split()[0] if current_price_text else \"\"  # ensures we get only \"$1014.30\"\n",
    "\n",
    "#                         # Extract discount if available (e.g., \"30% off\")\n",
    "#                         discount_el = await product.query_selector(\"span.tag-text\")\n",
    "#                         discount_text = await discount_el.inner_text() if discount_el else \"\"\n",
    "#                         #print(f\"Discount Text: {discount_text}\")  # Debugging\n",
    "#                         discount = discount_text.replace(\" off\", \"\").strip() if discount_text else \"\"  # just \"30%\"\n",
    "\n",
    "#                         # Extract original price with $ (if offer price is not available)\n",
    "#                         original_price_el = await product.query_selector(\"div.original-price\")\n",
    "#                         original_price_text = await original_price_el.inner_text() if original_price_el else \"\"\n",
    "#                         #print(f\"Original Price Text: {original_price_text}\")  # Debugging\n",
    "#                         original_price = original_price_text.strip().replace(\"Was\", \"\").strip().split()[0] if original_price_text else \"\"  # \"$1449.00\"\n",
    "\n",
    "#                         # Build the final formatted price\n",
    "#                         if current_price:  # If there is a current price\n",
    "#                             if discount:\n",
    "#                                 price = f\"{current_price} offer of {discount} {original_price}\"\n",
    "#                             else:\n",
    "#                                 price = current_price  # No discount, just current price\n",
    "#                         elif original_price:  # If there is no current price but original price is available\n",
    "#                             price = original_price\n",
    "#                         else:\n",
    "#                             price = \"N/A\"  # If neither price is available\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         price = \"N/A\"\n",
    "#                         print(f\"Error: {e}\")  # Log the error for debugging\n",
    "\n",
    "#                     try:\n",
    "#                         image_url = await (await product.query_selector(\"img[itemprop='image']\")).get_attribute(\"src\")\n",
    "#                     except:\n",
    "#                         image_url = \"N/A\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "#                     additional_info = []\n",
    "\n",
    "#                     try:\n",
    "#                         tag_els = await product.query_selector_all(\"span.product-tag.groupby-tablet-product-tags\")\n",
    "#                         if tag_els:\n",
    "#                             for tag_el in tag_els:\n",
    "#                                 tag_text = await tag_el.inner_text()\n",
    "#                                 if tag_text:\n",
    "#                                     additional_info.append(tag_text.strip())\n",
    "#                         else:\n",
    "#                             additional_info.append(\"N/A\")\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         additional_info.append(\"N/A\")\n",
    "\n",
    "#                     additional_info_str = \" | \".join(additional_info)    \n",
    "\n",
    "#                     print(\"=================== 1st phase data =============================\")\n",
    "\n",
    "#                     print(\"Row Number:\", row_num - 1)                          # int\n",
    "#                     print(\"Product URL:\", product_url)                     # str\n",
    "#                     print(\"Product Name:\", product_name)                   # str\n",
    "#                     print(\"Price:\", price)                                 # str or float\n",
    "#                     print(\"Image URL:\", image_url)                         # str or List[str]\n",
    "#                     print(\"Additional Info (Raw Text or JSON):\", additional_info_str)  # str (or JSON if parsed)\n",
    "                    \n",
    "#                     print(\"================================================\")\n",
    "\n",
    "\n",
    "#                     detail_data = await scrape_product_detail(product_url)\n",
    "                    \n",
    "#                     print(\"==================== second phase data IN ===========\")\n",
    "\n",
    "#                     print(\"title:\", detail_data[\"title\"])                      # str\n",
    "#                     print(\"sku:\", detail_data[\"sku\"])                          # str\n",
    "#                     print(\"ring_sizes:\", detail_data[\"ring_sizes\"])            # str or List[str]\n",
    "#                     print(\"final_output_price:\", detail_data[\"final_output_price\"])  # str or float\n",
    "#                     print(\"protection_plan:\", detail_data[\"protection_plan\"])  # str\n",
    "#                     print(\"monthly_payment:\", detail_data[\"monthly_payment\"])  # str\n",
    "#                     print(\"product_details:\", detail_data[\"product_details\"])  # str (OR JSON str if you convert it)\n",
    "#                     print(\"review_summary:\", detail_data[\"review_summary\"])    # str (OR JSON str if you convert it)\n",
    "#                     print(\"image_urls:\", detail_data[\"image_urls\"])            # List[str] or JSON str\n",
    "                    \n",
    "#                     print(\"==================== second phase data OUT ===========\")\n",
    " \n",
    "\n",
    "                    \n",
    "                    \n",
    "#                     if product_name == \"N/A\" or price == \"N/A\" or image_url == \"N/A\":\n",
    "#                         print(f\"Skipping product due to missing data: Name: {product_name}, Price: {price}, Image: {image_url}\")\n",
    "#                         continue    \n",
    "                    \n",
    "                    \n",
    "\n",
    "#                     gold_type_match = re.search(r\"\\b\\d{1,2}K\\s*(?:White|Yellow|Rose)?\\s*Gold\\b|\\bPlatinum\\b|\\bSilver\\b\", product_name, re.IGNORECASE)\n",
    "#                     kt = gold_type_match.group() if gold_type_match else \"Not found\"\n",
    "\n",
    "\n",
    "#                     diamond_weight_match = re.search(r\"\\d+(?:[-/]\\d+)?(?:\\s+\\d+/\\d+)?\\s*ct\\s+tw\", product_name, re.IGNORECASE)\n",
    "#                     diamond_weight = diamond_weight_match.group() if diamond_weight_match else \"N/A\"\n",
    "\n",
    "\n",
    "#                     unique_id = str(uuid.uuid4())\n",
    "#                     image_tasks.append((row_num, unique_id, asyncio.create_task(\n",
    "#                         download_image_async(image_url, product_name, timestamp, image_folder, unique_id)\n",
    "#                     )))\n",
    "\n",
    "#                     records.append((unique_id, current_date, page_title, product_name, None, kt, price, diamond_weight,additional_info_str))\n",
    "#                     sheet.append([current_date, page_title, product_name, None, kt, price, diamond_weight, time_only, image_url,additional_info_str])\n",
    "\n",
    "#                 # Process images and update records\n",
    "#                 for row_num, unique_id, task in image_tasks:\n",
    "#                     try:\n",
    "#                         image_path = await asyncio.wait_for(task, timeout=60)\n",
    "#                         if image_path != \"N/A\":\n",
    "#                             try:\n",
    "#                                 img = Image(image_path)\n",
    "#                                 img.width, img.height = 100, 100\n",
    "#                                 sheet.add_image(img, f\"D{row_num}\")\n",
    "#                             except Exception as img_error:\n",
    "#                                 logging.error(f\"Error adding image to Excel: {img_error}\")\n",
    "#                                 image_path = \"N/A\"\n",
    "                        \n",
    "#                         for i, record in enumerate(records):\n",
    "#                             if record[0] == unique_id:\n",
    "#                                 records[i] = (record[0], record[1], record[2], record[3], image_path, record[5], record[6], record[7], record[8])\n",
    "#                                 break\n",
    "#                     except asyncio.TimeoutError:\n",
    "#                         logging.warning(f\"Timeout downloading image for row {row_num}\")\n",
    "\n",
    "#                 all_records.extend(records)\n",
    "#                 success_count += 1\n",
    "\n",
    "#                 # Save progress after each page\n",
    "#                 wb.save(file_path)\n",
    "#                 logging.info(f\"Progress saved after page {page_count}\")\n",
    "#                 if page:\n",
    "#                     await page.close()\n",
    "#                 if browser:\n",
    "#                     await browser.close()\n",
    "                \n",
    "#                 page_count += 1\n",
    "#                 await asyncio.sleep(random.uniform(2, 5))\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error processing page {page_count}: {str(e)}\")\n",
    "#                 if page:\n",
    "#                     await page.close()\n",
    "#                 if browser:\n",
    "#                     await browser.close()\n",
    "#                 wb.save(file_path)\n",
    "#                 continue\n",
    "            \n",
    "#             # Add delay between pages\n",
    "#             await asyncio.sleep(random.uniform(2, 5))\n",
    "            \n",
    "#         page_count += 1\n",
    "\n",
    "#     # # Final save and database operations\n",
    "#     if not all_records:\n",
    "#         return None, None, None\n",
    "\n",
    "#     # Save the workbook\n",
    "#     wb.save(file_path)\n",
    "#     log_event(f\"Data saved to {file_path}\")\n",
    "\n",
    "#     # Encode the file in base64\n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         base64_encoded = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "#     # Insert data into the database and update product count\n",
    "#     insert_into_db(all_records)\n",
    "#     update_product_count(len(all_records))\n",
    "\n",
    "#     # Return necessary information\n",
    "#     return {\"status\": \"ok\"} \n",
    "\n",
    "# url = \"https://www.kay.com/engagement/c/9000000001?query=:_relevance_Ascending:v_gender_uFilter:%2522WOMEN%2522:v_custom_discount_percentage_string_uFilter:%252220%25%20%26%20up%2522:v_custom_discount_percentage_string_uFilter:%252210%25%20%26%20up%2522:v_stone1Shape_uFilter:%2522Oval%2522:v_Collection_uFilter:%2522Memories%20Moments%20Magic%2522:v_stone1CaratRange_uFilter:%25221%20Ctw%20-%20Under%203%20Ctw%2522:v_stone1CaratRange_uFilter:%25221%2F4%20Ctw%20-%20Under%201%2F2%20Ctw%2522&unbxdAPI=true\"\n",
    "# max_pages = 1\n",
    "\n",
    "# results = await handle_kay(url, max_pages)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086db1b-5e84-4bb4-ab40-c67ca9c106ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bda5c2-2f52-476b-885c-26bd11e3ee67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
